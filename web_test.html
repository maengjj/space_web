<!DOCTYPE html>
<html lang="ko"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><head><title>학술제 논문 제목 </title><meta http_quiv="content-type" content="text/html; charset=utf-8"><link rel="stylesheet" type="text/css" href="web_test_style.css"></head><body><div class="hpa" style="width:210mm;height:297mm;"><div class="hcD" style="left:30mm;top:40mm;"><div class="hfS" style="left:0mm;top:171.45mm;width:50mm;height:0.11mm;"><svg class="hs" viewBox="-0.12 -0.12 50.23 0.35" style="left:-0.12mm;top:-0.12mm;width:50.23mm;height:0.35mm;left:0;top:0;"><path d="M0,0.06 L50,0.06" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path></svg></div><div class="hcD" style="left:0mm;top:173.51mm;"><div class="hcI"><div class="hls ps1" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:-0.16mm;height:3.17mm;width:134mm;"><span class="hrt cs3">지도교수 : 김 도 형&nbsp;</span></div><div class="hls ps1" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:3.98mm;height:3.17mm;width:134mm;"><div class="haN" style="left:0mm;top:0mm;width:2.92mm;height:3.17mm;"><span class="hrt cs3">1)</span></div><span class="hrt cs3">&nbsp;rlatg0123@naver.com&nbsp;</span></div></div></div><div class="hcD" style="left:0mm;top:181.82mm;"><div class="hcI"><div class="hls ps1" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:-0.16mm;height:3.17mm;width:134mm;"><div class="haN" style="left:0mm;top:0mm;width:2.92mm;height:3.17mm;"><span class="hrt cs3">2)</span></div><span class="hrt cs3">&nbsp;mjiju06@naver.com</span></div></div></div><div class="hcI"><div class="hls ps24" style="padding-left:3.53mm;line-height:14.67mm;white-space:nowrap;left:0mm;top:0mm;height:14.67mm;width:134mm;"><div class="htb" style="left:0mm;width:56.18mm;top:0mm;height:14.67mm;display:inline-block;position:relative;vertical-align:middle;"><svg class="hs" viewBox="-2.50 -2.50 61.18 19.67" style="left:-2.50mm;top:-2.50mm;width:61.18mm;height:19.67mm;"><path d="M-0.06,-0.17 L56.24,-0.17" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.35;"></path><path d="M-0.06,0.26 L56.24,0.26" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.17;"></path><path d="M-0.06,14.40 L56.24,14.40" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.17;"></path><path d="M-0.06,14.84 L56.24,14.84" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.35;"></path></svg><div class="hce" style="left:0mm;top:0mm;width:56.18mm;height:14.67mm;"><div class="hcD" style="left:0.50mm;top:0.50mm;"><div class="hcI"><div class="hls ps22" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:-0.16mm;height:3.17mm;width:55.17mm;"><span class="hrt cs18">마세마타 제??권(2023년)</span></div><div class="hls ps22" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:3.34mm;height:3.17mm;width:55.17mm;"><span class="hrt cs18">단국대학교 수학과 학술제 발표논문</span></div><div class="hls ps22" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:6.84mm;height:3.17mm;width:55.17mm;"><span class="hrt cs17">&nbsp;Mathematics of Dankook University</span></div><div class="hls ps22" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:10.34mm;height:3.17mm;width:55.17mm;"><span class="hrt cs19">Vol.?? December 2023&nbsp;</span><span class="hrt cs20">??~??</span></div></div></div></div></div></div><div class="hls ps21" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:14.51mm;height:3.17mm;width:134mm;"></div><div class="hls ps21" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:17.68mm;height:3.17mm;width:134mm;"></div><div class="hls ps21" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:20.86mm;height:3.17mm;width:134mm;"></div><div class="hls ps21" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:24.03mm;height:3.17mm;width:134mm;"></div><div class="hls ps21" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:27.21mm;height:3.17mm;width:134mm;"></div><div class="hls ps21" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:30.38mm;height:3.17mm;width:134mm;"></div><div class="hls ps14" style="line-height:5.52mm;white-space:nowrap;left:0mm;top:33.40mm;height:6.35mm;width:134mm;"><span class="hrt cs23">천체 분류를 위한 머신러닝 기법의</span></div><div class="hls ps14" style="line-height:5.52mm;white-space:nowrap;left:0mm;top:44.83mm;height:6.35mm;width:134mm;"><span class="hrt cs23">성능 평가 연구</span><span class="hrt cs14">&nbsp;</span></div><div class="hls ps15" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:56.40mm;height:3.53mm;width:134mm;"></div><div class="hls ps15" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:59.93mm;height:3.53mm;width:134mm;"></div><div class="hls ps15" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:63.46mm;height:3.53mm;width:134mm;"></div><div class="hls ps12" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:66.99mm;height:3.53mm;width:134mm;"><span class="hrt cs1">단국대학교 수학과&nbsp;</span></div><div class="hls ps12" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:72.63mm;height:3.53mm;width:134mm;"><span class="hrt cs1">32191235 김택균</span><span class="hfN" style="top:-1.76mm;"><span class="hrt cs1" style="font-size:5pt;top:-1pt;">1)</span></span><span class="hrt cs1">, 32191468 맹지주</span><span class="hfN" style="top:-1.76mm;"><span class="hrt cs1" style="font-size:5pt;top:-1pt;">2)</span></span></div><div class="hls ps21" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:78.27mm;height:3.53mm;width:134mm;"><span class="hrt cs1">&nbsp;</span><span class="hrt cs9">&nbsp;&nbsp;</span></div><div class="hls ps21" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:81.82mm;height:3.17mm;width:134mm;"></div><div class="hls ps12" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:84.98mm;height:3.53mm;width:134mm;"></div><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:90.62mm;height:3.53mm;width:134mm;"><span class="hrt cs1">&nbsp;</span><span class="hrt cs9">&nbsp;</span></div><div class="hls ps2" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:96.27mm;height:3.53mm;width:134mm;"></div><div class="hls ps16" style="line-height:2.79mm;white-space:nowrap;left:8.82mm;top:103.91mm;height:3.53mm;width:116.36mm;"><span class="hrt cs13">요 약</span><span class="hrt cs9">&nbsp;</span><span class="hrt cs9">:</span><span class="hrt cs11">&nbsp;</span><span class="hrt cs9">본 연구는 인공지능과 관련된 과제를 제시해주고 참가자가 이를 해결하</span></div><div class="hls ps16" style="line-height:2.48mm;white-space:nowrap;left:8.82mm;top:109.58mm;height:3.17mm;width:116.36mm;"><span class="hrt cs9">는 데이콘(DACON)이 주최한 대회인 ‘월간 데이콘 천체 유형 분류 대회’ 문제</span></div><div class="hls ps16" style="line-height:2.48mm;white-space:nowrap;left:8.82mm;top:114.66mm;height:3.17mm;width:116.36mm;"><span class="hrt cs9">를 바탕으로 데이터 전처리와 모델 구축 및 평가를 하여 다양한 각도에서 문제</span></div><div class="hls ps16" style="line-height:2.48mm;white-space:nowrap;left:8.82mm;top:119.74mm;height:3.17mm;width:116.36mm;"><span class="hrt cs9">를 바라보고자 한다. 특히, AutoML 라이브러리인&nbsp;</span><span class="hrt cs34">PyCaret를 이용하여 각 모</span></div><div class="hls ps16" style="line-height:2.48mm;white-space:nowrap;left:8.82mm;top:124.82mm;height:3.17mm;width:116.36mm;"><span class="hrt cs34">델의 정확도를 평가하고</span><span class="hrt cs9">&nbsp;모델들을 앙상블하여 다양한 모델들을 비교 분석하고,&nbsp;</span></div><div class="hls ps16" style="line-height:2.48mm;white-space:nowrap;left:8.82mm;top:129.90mm;height:3.17mm;width:116.36mm;"><span class="hrt cs34">주요 모델의 선택과 튜닝에 대한 통찰</span><span class="hrt cs9">을 얻고자 한다. 본 연구가 수학 및 데이</span></div><div class="hls ps16" style="line-height:2.48mm;white-space:nowrap;left:8.82mm;top:134.98mm;height:3.17mm;width:116.36mm;"><span class="hrt cs9">터 사이언스에 관심 있는 학생들과 입문자들에게 많은 도움이 되었으면 한다.</span></div><div class="hls ps16" style="line-height:2.48mm;white-space:nowrap;left:8.82mm;top:142.06mm;height:3.17mm;width:116.36mm;"></div><div class="hls ps16" style="line-height:2.48mm;white-space:nowrap;left:8.82mm;top:149.14mm;height:3.17mm;width:116.36mm;"></div><div class="hls ps16" style="line-height:2.48mm;white-space:nowrap;left:8.82mm;top:156.23mm;height:3.17mm;width:116.36mm;"></div><div class="hls ps23" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:163.31mm;height:3.17mm;width:128.71mm;"></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:161.97mm;top:33.53mm;width:2.03mm;height:3.53mm;"><span class="hrt cs0">2</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hcI"><div class="hls ps2" style="line-height:4.10mm;white-space:nowrap;left:0mm;top:-0.25mm;height:4.94mm;width:134mm;"><span class="hrt cs7">1. 서론</span><span class="hrt cs8">&nbsp;</span></div><div class="hls ps29" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:9.49mm;height:3.53mm;width:134mm;"><span class="hrt cs6">머신러닝 알</span><span class="hrt cs1">고리즘은 데이터를 기반으로 통계적인 신뢰도를 강화하고 예측 오</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:15.84mm;height:3.53mm;width:134mm;"><span class="hrt cs1">류를 최소화하기 위한 다양한 수학적 기법을 적용해 데이터 내의 패턴을 스스로&nbsp;</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:22.19mm;height:3.53mm;width:134mm;"><span class="hrt cs1">인지하고 신뢰도 있는 예측 결과를 도출해 낸다. 이러한 머신러닝에는 크게 지도</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:28.54mm;height:3.53mm;width:134mm;"><span class="hrt cs1">학습과 비지도학습으로 나뉜다. 지도학습에는 대표적으로 분류와 회귀가 있는데,&nbsp;</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:34.89mm;height:3.53mm;width:134mm;"><span class="hrt cs1">본 연구는 분류 모델을 활용하여 천체의 유형을 구분(분류)하는 알고리즘 모델을&nbsp;</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:41.24mm;height:3.53mm;width:134mm;"><span class="hrt cs1">만들 계획이다. 분류 모델이란, 기존에 존재하는 데이터의 카테고리를 파악하고,&nbsp;</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:47.59mm;height:3.53mm;width:134mm;"><span class="hrt cs1">새롭게 관측된 데이터의 카테고리를 스스로 판별하는 과정을 말한다. 분류 모델은&nbsp;</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:53.94mm;height:3.53mm;width:134mm;"><span class="hrt cs1">입력 데이터를 미리 정의된 클래스 또는 범주로 할당하는 작업에 사용되며, 입력&nbsp;</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:60.29mm;height:3.53mm;width:134mm;"><span class="hrt cs1">데이터를 여러 클래스 중 하나로 분류하는 것이 목표이다. 이진 분류를 통해 참-</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:66.64mm;height:3.53mm;width:134mm;"><span class="hrt cs1">거짓을 판별할 수도 있지만, 다중 분류를 통해 3개의 이상의 값을 판별할 수 있</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:72.99mm;height:3.53mm;width:134mm;"><span class="hrt cs1">다. 위 분류 모델을 활용하기 위해 '데이콘'이라는 인공지능 경진대회 플랫폼의 주</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:79.34mm;height:3.53mm;width:134mm;"><span class="hrt cs1">제와 데이터셋을 활용하기로 하였다.&nbsp;</span></div><div class="hls ps30" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:87.45mm;height:3.53mm;width:134mm;"><span class="hrt cs1">‘데이콘'은 온라인 데이터 사이언스 경진 대회와 교육을 함께 진행할 수 있는&nbsp;</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:93.80mm;height:3.53mm;width:134mm;"><span class="hrt cs1">플랫폼으로 다양한 기업의 데이터를 받아 사용자들에게 이를 학습 등 활용할 수&nbsp;</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:100.15mm;height:3.53mm;width:134mm;"><span class="hrt cs1">있는 공간을 제공하고 그곳에서 나온 좋은 프로그램을 다시 데이터 제공 기관에</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:106.50mm;height:3.53mm;width:134mm;"><span class="hrt cs1">게 전달하여 기술 개발의 단초를 제공해 준다.</span></div><div class="hls ps30" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:114.62mm;height:3.53mm;width:134mm;"><span class="hrt cs1">본 연구에서는 데이콘에서 제공한 '천체 유형 분류' 데이터를 사용하여 분류 모</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:120.97mm;height:3.53mm;width:134mm;"><span class="hrt cs1">델에 적용시켰다. '천체 유형 분류' 데이터를 통해서 천체의 유형을 분류함으로써&nbsp;</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:127.32mm;height:3.53mm;width:134mm;"><span class="hrt cs1">우주의 현상과 구조를 밝혀내는 데 도움을 줄 수 있고, 우주의 다양한 현상을 이</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:133.67mm;height:3.53mm;width:134mm;"><span class="hrt cs1">해하는 데 도움이 될 수 있다. 본 연구에서는 Python의 AutoML 라이브러리인&nbsp;</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:140.02mm;height:3.53mm;width:134mm;"><span class="hrt cs1">Pycaret를 사용한다. 천체 관측을 통해 측정된 21개의 데이터를 이용하여 이미&nbsp;</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:146.37mm;height:3.53mm;width:134mm;"><span class="hrt cs1">정의된 19개의 천체 유형을 분류하는 알고리즘을 개발한다.&nbsp;</span></div><div class="hls ps30" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:154.48mm;height:3.53mm;width:134mm;"></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:30mm;top:33.53mm;width:2.03mm;height:3.53mm;"><span class="hrt cs0">3</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hfS" style="left:0mm;top:155.91mm;width:50mm;height:0.11mm;"><svg class="hs" viewBox="-0.12 -0.12 50.23 0.35" style="left:-0.12mm;top:-0.12mm;width:50.23mm;height:0.35mm;left:0;top:0;"><path d="M0,0.06 L50,0.06" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path></svg></div><div class="hcD" style="left:0mm;top:157.96mm;"><div class="hcI"><div class="hls ps1" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:-0.16mm;height:3.17mm;width:134mm;"><div class="haN" style="left:0mm;top:0mm;width:2.92mm;height:3.17mm;"><span class="hrt cs3">3)</span></div><span class="hrt cs3">&nbsp;cost function 이란 머신러닝 또는 인공지능 알고리즘에서 가설함수가 얼마나 정확한지 판단하는&nbsp;</span></div><div class="hls ps1" style="padding-left:4.62mm;line-height:2.48mm;white-space:nowrap;left:0mm;top:3.98mm;height:3.17mm;width:134mm;"><span class="hrt cs3">함수로 다음과 같이 작성한다.</span></div><div class="hls ps1" style="line-height:10.31mm;white-space:nowrap;left:0mm;top:8.27mm;height:10.45mm;width:134mm;"><div class="heq" style="width:57.42mm;height:10.45mm;background-repeat:no-repeat;background-image:url('web_test_hd1.png');display:inline-block;position:relative;vertical-align:middle;"></div><span class="hrt cs29">&nbsp;이며, 이때 &nbsp;</span><div class="heq" style="width:24.56mm;height:3.97mm;background-repeat:no-repeat;background-image:url('web_test_hd2.png');display:inline-block;position:relative;vertical-align:middle;"></div><span class="hrt cs29">를 뜻한다.</span></div></div></div><div class="hcD" style="left:0mm;top:177.68mm;"><div class="hcI"><div class="hls ps1" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:-0.16mm;height:3.17mm;width:134mm;"><div class="haN" style="left:0mm;top:0mm;width:2.92mm;height:3.17mm;"><span class="hrt cs3">4)</span></div><span class="hrt cs3">&nbsp;</span><span class="hrt cs29">로지스틱 회귀분석을 활용한 분류(classification) 문제에서는 목표변수를 직접 예측(prediction)하</span></div><div class="hls ps1" style="padding-left:4.62mm;line-height:2.48mm;white-space:nowrap;left:0mm;top:3.98mm;height:3.17mm;width:134mm;"><span class="hrt cs29">는 것이 아닌 2개의 값('성공(1)' or '실패(0)') 중 하나의 값으로 예측할 때 사용된다.</span></div></div></div><div class="hcI"><div class="hls ps2" style="line-height:4.10mm;white-space:nowrap;left:0mm;top:-0.25mm;height:4.94mm;width:134mm;"><span class="hrt cs26">2. 지도학습에서 분류</span></div><div class="hls ps30" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:9.49mm;height:3.53mm;width:134mm;"><span class="hrt cs1">데이터 분석 모델은 학습 유형에 따라 지도학습과 비지도학습, 준지도학습, 강</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:15.84mm;height:3.53mm;width:134mm;"><span class="hrt cs1">화학습으로 분류된다. 지도학습이란 훈련 데이터로부터 하나의 함수를 유추해 내</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:22.19mm;height:3.53mm;width:134mm;"><span class="hrt cs1">기 위한 기계 학습의 한 방법으로 예시를 통해 학습하도록 설계된다.&nbsp;</span><span class="hrt cs6">즉, 간단히&nbsp;</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:28.54mm;height:3.53mm;width:134mm;"><span class="hrt cs6">말해 선생님이 문제를 내고 그다음 바로 정답까지 같이 알려주는 방식의 학습 방</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:34.89mm;height:3.53mm;width:134mm;"><span class="hrt cs6">법인 것이다. 정답이 있는 데이터의 학습을 의미한다.&nbsp;</span><span class="hrt cs1">지도학습에서 분류</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:41.24mm;height:3.53mm;width:134mm;"><span class="hrt cs1">(Classifier)는 기존에 존재하는 데이터 안에 있는 카테고리 간의 관계를 파악하</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:47.59mm;height:3.53mm;width:134mm;"><span class="hrt cs1">고,</span><span class="hrt cs25">&nbsp;새롭게 관측된 데이터의 카테고리를 스스로 판별하는 과정</span><span class="hrt cs1">이다. 이러한 분류</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:53.94mm;height:3.53mm;width:134mm;"><span class="hrt cs1">를 하기 위한 모델을 분류모델이라고 한다.</span></div><div class="hls ps30" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:62.05mm;height:3.53mm;width:134mm;"><span class="hrt cs1">분류에서&nbsp;</span><span class="hfN" style="top:-1.76mm;"><span class="hrt cs1" style="font-size:5pt;top:-1pt;">3)</span></span><span class="hrt cs1">cost function은 회귀모델에서와 마찬가지로 분류모델이 얼마나 잘&nbsp;</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:68.40mm;height:3.53mm;width:134mm;"><span class="hrt cs1">예측하는지를 알 수 있어야 하는데 그 정도를 측정할 수 있는 함수이다. 하지만&nbsp;</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:74.75mm;height:3.53mm;width:134mm;"><span class="hrt cs1">회귀모델에서의 cost function의 값은 따로 제한이 없지만&nbsp;</span><span class="hfN" style="top:-1.76mm;"><span class="hrt cs1" style="font-size:5pt;top:-1pt;">4)</span></span><span class="hrt cs1">분류모델에서는 그&nbsp;</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:81.10mm;height:3.53mm;width:134mm;"><span class="hrt cs1">값이 0과 1사이의 값으로 나와야 한다. 따라서 이러한 결과 값을 나오게 하기 위</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:87.45mm;height:3.53mm;width:134mm;"><span class="hrt cs1">해서 활성화 함수(activation function)를 활용한다. 신경망에서는 노드에 들어오</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:93.80mm;height:3.53mm;width:134mm;"><span class="hrt cs1">는 값들에 대해 곧바로 다음 레이어로 전달하지 않고 활성화 함수를 통과시킨 후&nbsp;</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:100.15mm;height:3.53mm;width:134mm;"><span class="hrt cs1">전달한다. 활성화 함수는 입력 신호의 총합을 출력 신호로 변환하는 함수로, 입력</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:106.50mm;height:3.53mm;width:134mm;"><span class="hrt cs1">받은 신호를 얼마나 출력할지 결정하고 네트워크에 층을 쌓아 비선형성을 표현할&nbsp;</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:112.85mm;height:3.53mm;width:134mm;"><span class="hrt cs1">수 있도록 해준다.&nbsp;</span></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:161.97mm;top:33.53mm;width:2.03mm;height:3.53mm;"><span class="hrt cs0">4</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hfS" style="left:0mm;top:179.76mm;width:50mm;height:0.11mm;"><svg class="hs" viewBox="-0.12 -0.12 50.23 0.35" style="left:-0.12mm;top:-0.12mm;width:50.23mm;height:0.35mm;left:0;top:0;"><path d="M0,0.06 L50,0.06" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path></svg></div><div class="hcD" style="left:0mm;top:181.82mm;"><div class="hcI"><div class="hls ps1" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:-0.16mm;height:3.17mm;width:134mm;"><div class="haN" style="left:0mm;top:0mm;width:2.92mm;height:3.17mm;"><span class="hrt cs3">5)</span></div><span class="hrt cs3">&nbsp;[출처]&nbsp;</span><span class="hrt cs29">https://untitledtblog.tistory.com/85</span></div></div></div><div class="hcI"><div class="hls ps31" style="line-height:4.10mm;white-space:nowrap;left:0mm;top:-0.25mm;height:4.94mm;width:134mm;"><span class="hrt cs26">3. 사용된 알고리즘</span></div><div class="hls ps31" style="line-height:4.10mm;white-space:nowrap;left:0mm;top:7.66mm;height:4.94mm;width:134mm;"></div><div class="hls ps31" style="line-height:3.43mm;white-space:nowrap;left:0mm;top:15.59mm;height:4.23mm;width:134mm;"><span class="hrt cs32">그래프</span><span class="hfN" style="top:-2.12mm;"><span class="hrt cs32" style="font-size:6pt;top:-1.20pt;">5)</span></span></div><div class="hls ps30" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:24.17mm;height:3.53mm;width:134mm;"><span class="hrt cs6">앞으로 설명할 모델들은 모두 트리 구조를 가진다는 공통점이 있다. 그리고 이</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:30.52mm;height:3.53mm;width:134mm;"><span class="hrt cs6">러한 트리 구조는 이산수학의 그래프 이론에서도 중요한 부분이기에 이에 대한&nbsp;</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:36.87mm;height:3.53mm;width:134mm;"><span class="hrt cs6">설명이 필요하다.</span></div><div class="hls ps30" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:44.98mm;height:3.53mm;width:134mm;"><span class="hrt cs6">트리의 개념을 이해하기 위해서는 먼저 이산 수학(discrete mathematics)의 그</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:51.33mm;height:3.53mm;width:134mm;"><span class="hrt cs6">래프(graph)를 이해할 필요가 있다. 그래프 G는 G=(V,E)와 같이 표현되며, V는&nbsp;</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:57.68mm;height:3.53mm;width:134mm;"><span class="hrt cs6">그래프의 노드를 나타내는 집합이고, E는 그래프의 간선을 나타내는 집합이다. 이</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:64.03mm;height:3.53mm;width:134mm;"><span class="hrt cs6">때의 노드는 그래프에서 데이터를 저장하는 기본 단위로 Vertex라고도 표현하며,&nbsp;</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:70.38mm;height:3.53mm;width:134mm;"><span class="hrt cs6">그래프 내의 개별 항목을 나타낸다. 또한 간선은 노드 간의 관계를 나타내고&nbsp;</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:76.73mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Edge라고 표현하며 두 노드를 연결하는 선을 의미한다. 아래의 [그림1]은 그래프</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:83.08mm;height:3.53mm;width:134mm;"><span class="hrt cs6">에 대한 몇 가지 예시를 나타낸다.</span></div><div class="hls ps35" style="padding-left:3.53mm;line-height:48.48mm;white-space:nowrap;left:0mm;top:91.37mm;height:48.48mm;width:134mm;"><div class="hsG" style="width:114.68mm;height:41.96mm;display:inline-block;position:relative;vertical-align:middle;"><div class="hsR" style="left:0mm;margin-right:0mm;width:114.68mm;height:41.96mm;background-repeat:no-repeat;background-image:url('web_test_hd3.png');"></div><div class="hcD" style="left:0mm;top:44.95mm;width:114.68mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:114.68mm;"><span class="hrt cs1">[그림&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:2.10mm;height:3.53mm;"><span class="hrt cs1">1</span></div><span class="hrt cs1">]</span></div></div></div></div><span class="hrt cs6">.</span></div><div class="hls ps29" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:145.39mm;height:3.53mm;width:134mm;"><span class="hrt cs6">이러한 그래프에는 Walk와 Path 그리고 Cycle이라는 개념이 존재한다. Walk는&nbsp;</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:151.74mm;height:3.53mm;width:134mm;"><span class="hrt cs6">그래프 내에서 Vertex와 Edge를 번갈아 가면서 순회하는 경로를 의미하고, Path</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:158.09mm;height:3.53mm;width:134mm;"><span class="hrt cs6">는 Edge가 반복되지 않는 Walk를 의미하며, Cycle은 경로의 시작 Vertex와 마</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:164.44mm;height:3.53mm;width:134mm;"><span class="hrt cs6">지막 Vertex가 같은 Path를 의미한다. 아래의 [그림2]를 통해서 예를 들어보겠</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:170.79mm;height:3.53mm;width:134mm;"><span class="hrt cs6">다.</span></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:30mm;top:33.53mm;width:2.03mm;height:3.53mm;"><span class="hrt cs0">5</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hcI"><div class="hls ps36" style="padding-left:3.53mm;line-height:61.53mm;white-space:nowrap;left:0mm;top:1.76mm;height:61.53mm;width:134mm;"><div class="hsG" style="width:61.78mm;height:55.01mm;display:inline-block;position:relative;vertical-align:middle;"><div class="hsR" style="left:0mm;margin-right:0mm;width:61.78mm;height:55.01mm;background-repeat:no-repeat;background-image:url('web_test_hd4.png');"></div><div class="hcD" style="left:0mm;top:58mm;width:61.78mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:61.78mm;"><span class="hrt cs1">[그림&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:2.10mm;height:3.53mm;"><span class="hrt cs1">2</span></div><span class="hrt cs1">]</span></div></div></div></div></div><div class="hls ps29" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:68.83mm;height:3.53mm;width:134mm;"><span class="hrt cs6">만약 [그림2]의 경로가 A-B-C-A라고 한다면 이는 Vertex와 Edge를 번갈아&nbsp;</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:75.18mm;height:3.53mm;width:134mm;"><span class="hrt cs6">가면서 순회하는 경로이기에 Walk이고, 반복되는 Edge 없이 Walk의 조건을 만족</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:81.53mm;height:3.53mm;width:134mm;"><span class="hrt cs6">했으므로 Path의 조건도 만족한다. 또한 경로의 시작 Vertex와 마지막 Vertex가&nbsp;</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:87.88mm;height:3.53mm;width:134mm;"><span class="hrt cs6">모두 A로 같기에 Cycle의 조건 또한 만족한다.</span></div><div class="hls ps29" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:96mm;height:3.53mm;width:134mm;"><span class="hrt cs6">그렇다면 경로가 A-B-C-A-B라면 어떨까? 시작 Vertex는 A이고 마지막&nbsp;</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:102.35mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Vertex는 B이므로 서로 달라서 이는 Cycle이 아니고 A-B의 경로에서 반복되는&nbsp;</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:108.70mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Edge가 생겼으므로 이는 Path라고도 할 수 없는 것이다.</span></div><div class="hls ps29" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:116.81mm;height:3.53mm;width:134mm;"></div><div class="hls ps29" style="padding-left:3.53mm;line-height:3.43mm;white-space:nowrap;left:0mm;top:124.89mm;height:4.23mm;width:134mm;"><span class="hrt cs33">트리</span></div><div class="hls ps29" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:134.31mm;height:3.53mm;width:134mm;"><span class="hrt cs6">이산 수학 또는 그래프 이론에서 트리는 어떤 두 Vertex가 정확히 하나의 유일</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:140.66mm;height:3.53mm;width:134mm;"><span class="hrt cs6">한 Path로만 연결된 그래프를 의미한다. 두 Vertex가 유일한 Path로만 연결되어&nbsp;</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:147.01mm;height:3.53mm;width:134mm;"><span class="hrt cs6">있다는 것은 그래프에 Cycle이 존재하지 않는다는 것과 동치이다. 아래 [그림3]</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:153.36mm;height:3.53mm;width:134mm;"><span class="hrt cs6">을 통해 예를 들겠다.&nbsp;</span></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:161.97mm;top:33.53mm;width:2.03mm;height:3.53mm;"><span class="hrt cs0">6</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hcI"><div class="hls ps36" style="padding-left:3.53mm;line-height:60.78mm;white-space:nowrap;left:0mm;top:1.76mm;height:60.78mm;width:134mm;"><div class="hsG" style="width:119.34mm;height:54.26mm;display:inline-block;position:relative;vertical-align:middle;"><div class="hsR" style="left:0mm;margin-right:0mm;width:119.34mm;height:54.26mm;background-repeat:no-repeat;background-image:url('web_test_hd5.png');"></div><div class="hcD" style="left:0mm;top:57.26mm;width:119.34mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:119.34mm;"><span class="hrt cs1">[그림&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:2.10mm;height:3.53mm;"><span class="hrt cs1">3</span></div><span class="hrt cs1">]</span></div></div></div></div></div><div class="hls ps29" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:68.09mm;height:3.53mm;width:134mm;"><span class="hrt cs31">&nbsp;&nbsp;&nbsp;&nbsp;</span></div><div class="hls ps29" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:76.20mm;height:3.53mm;width:134mm;"><span class="hrt cs6">[그림3]에서 (b)는 A에서 C로 이동하는 path가 2개(A-B-C, A-C) 존재하므로&nbsp;</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:82.55mm;height:3.53mm;width:134mm;"><span class="hrt cs6">어떤 두 Vertex가 정확히 하나의 유일한 Path로만 연결되어야 한다는 조건에 맞</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:88.90mm;height:3.53mm;width:134mm;"><span class="hrt cs6">지 않아서 트리가 아니다. 또는 A-B-C-A로 이루어진 Cycle이 존재하므로 트리</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:95.25mm;height:3.53mm;width:134mm;"><span class="hrt cs6">가 아니라고도 할 수 있다.</span></div><div class="hls ps29" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:103.36mm;height:3.53mm;width:134mm;"><span class="hrt cs6">(c)는 Cycle이 존재하지는 않지만, A 또는 B에서 C로 이동하는 path가 존재하</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:109.71mm;height:3.53mm;width:134mm;"><span class="hrt cs6">지 않는다. 이또한 어떤 두 Vertex 사이에 하나의 유일한 Path가 존재해야한다는&nbsp;</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:116.06mm;height:3.53mm;width:134mm;"><span class="hrt cs6">조건에 맞지 않으므로 (c) 또한 트리가 아니다.</span></div><div class="hls ps29" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:124.18mm;height:3.53mm;width:134mm;"><span class="hrt cs6">마지막으로 (a)는 어떤 두 Vertex가 정확히 하나의 유일한 Path로만 연결된 그</span></div><div class="hls ps29" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:130.53mm;height:3.53mm;width:134mm;"><span class="hrt cs6">래프라는 조건에 완벽하게 부합하므로 트리라고 할 수 있다.</span></div><div class="hls ps30" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:138.64mm;height:3.53mm;width:134mm;"></div><div class="hls ps30" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:146.76mm;height:3.53mm;width:134mm;"><span class="hrt cs1">본 연구에서는 위에서 설명한 트리 구조를 기반으로 하는 다음의 5가지 알고리</span></div><div class="hls ps30" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:153.11mm;height:3.53mm;width:134mm;"><span class="hrt cs1">즘을 사용한다.</span></div><div class="hls ps30" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:161.22mm;height:3.53mm;width:134mm;"></div><div class="hls ps32" style="padding-left:3.53mm;line-height:4.10mm;white-space:nowrap;left:0mm;top:169.26mm;height:4.94mm;width:134mm;"><span class="hrt cs7">3.1 랜덤 포레스트 (Random Forest)</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:179.99mm;height:3.53mm;width:134mm;"><span class="hrt cs6">랜덤 포레스트(RandomForest)는 여러 개의 결정트리(Decision Tree)를 활용한&nbsp;</span></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:30mm;top:33.53mm;width:2.03mm;height:3.53mm;"><span class="hrt cs0">7</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hfS" style="left:0mm;top:171.49mm;width:50mm;height:0.11mm;"><svg class="hs" viewBox="-0.12 -0.12 50.23 0.35" style="left:-0.12mm;top:-0.12mm;width:50.23mm;height:0.35mm;left:0;top:0;"><path d="M0,0.06 L50,0.06" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path></svg></div><div class="hcD" style="left:0mm;top:173.55mm;"><div class="hcI"><div class="hls ps1" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:-0.16mm;height:3.17mm;width:134mm;"><div class="haN" style="left:0mm;top:0mm;width:2.92mm;height:3.17mm;"><span class="hrt cs3">6)</span></div><span class="hrt cs3">[출처]</span></div><div class="hls ps1" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:3.98mm;height:3.17mm;width:134mm;"><span class="hrt cs29">https://velog.io/@yuns_u/%EA%B2%B0%EC%A0%95%ED%8A%B8%EB%A6%AC%EC%9D%98%E</span></div><div class="hls ps1" style="padding-left:4.62mm;line-height:2.48mm;white-space:nowrap;left:0mm;top:8.11mm;height:3.17mm;width:134mm;"><span class="hrt cs29">C%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4-Decision-Trees</span></div></div></div><div class="hcI"><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:134mm;"><span class="hrt cs6">배깅방식의 대표적인 알고리즘이다. 여기서 결정트리(Decision Tree)와 배깅</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:6.17mm;height:3.53mm;width:134mm;"><span class="hrt cs6">(bagging)에 대해서 설명이 필요하다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:4.10mm;white-space:nowrap;left:0mm;top:14.22mm;height:4.94mm;width:134mm;"></div><div class="hls ps32" style="padding-left:3.53mm;line-height:4.10mm;white-space:nowrap;left:0mm;top:24.87mm;height:4.94mm;width:134mm;"><span class="hrt cs26">3.1.1 결정 트리 (Decision Tree)</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:35.60mm;height:3.53mm;width:134mm;"><span class="hrt cs6">특정 기준(질문)에 따라 데이터를 구분하는 모델을 결정 트리 모델이라고 한다.&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:41.95mm;height:3.53mm;width:134mm;"><span class="hrt cs6">결정 트리에서 질문이나 정답을 담은 네모 상자를 노드(Node)라고 하고, 맨 처음&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:48.30mm;height:3.53mm;width:134mm;"><span class="hrt cs6">분류 기준 (즉, 첫 질문)을 Root Node, 맨 마지막 노드를 Terminal Node 혹은&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:54.65mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Leaf Node라고 한다.</span></div><div class="hls ps33" style="padding-left:3.53mm;line-height:94.16mm;white-space:nowrap;left:0mm;top:62.94mm;height:94.16mm;width:134mm;"><div class="hsG" style="width:83.99mm;height:88.64mm;display:inline-block;position:relative;vertical-align:middle;"><div class="hsR" style="left:0mm;margin-right:0mm;width:83.99mm;height:88.64mm;background-repeat:no-repeat;background-image:url('web_test_hd6.png');"></div><div class="hcD" style="left:0mm;top:90.63mm;width:83.99mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:83.99mm;"><span class="hrt cs1">[그림&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:2.10mm;height:3.53mm;"><span class="hrt cs1">4</span></div><span class="hrt cs1">]</span></div></div></div></div></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:161.50mm;height:3.53mm;width:134mm;"><span class="hrt cs6">위의 [그림 4]을 보면 이해가 더 쉬울 것이다.</span><span class="hfN" style="top:-1.76mm;"><span class="hrt cs6" style="font-size:5pt;top:-1pt;">6)</span></span></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:161.97mm;top:33.53mm;width:2.03mm;height:3.53mm;"><span class="hrt cs0">8</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hcI"><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:1.59mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Root node에서 질문을 시작하여 질문이 참이면 YES를 통해서, 거짓이면 NO를&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:7.94mm;height:3.53mm;width:134mm;"><span class="hrt cs6">통해서 다음 노드로 데이터를 보내는 방식이다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:16.05mm;height:3.53mm;width:134mm;"></div><div class="hls ps32" style="padding-left:3.53mm;line-height:4.10mm;white-space:nowrap;left:0mm;top:24.09mm;height:4.94mm;width:134mm;"><span class="hrt cs7">3.1.2 배깅 (Bagging)</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:34.82mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Bagging은 Bootstrap Aggregation의 약자로 복원추출을 통해서 샘플을 여러&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:41.17mm;height:3.53mm;width:134mm;"><span class="hrt cs6">번 랜덤하게 뽑아서 뽑은 모델들을 각각 학습시켜 그 결과물을 집계하는 방법이</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:47.52mm;height:3.53mm;width:134mm;"><span class="hrt cs6">다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:55.63mm;height:3.53mm;width:134mm;"><span class="hrt cs6">배깅은 아래와 같은 방식으로 진행된다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:63.75mm;height:3.53mm;width:134mm;"><span class="hrt cs6">(1) 동일한 알고리즘을 사용하는 일정 수의 분류기 생성</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:71.86mm;height:3.53mm;width:134mm;"><span class="hrt cs6">(2) 각각의 분류기는 부트스트래핑(Bootstrapping)방식으로 생성된 샘플 데이터</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:78.21mm;height:3.53mm;width:134mm;"><span class="hrt cs6">를 학습</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:86.32mm;height:3.53mm;width:134mm;"><span class="hrt cs6">(3) 최종적으로 모든 분류기의 결과를 집계해서 예측값 결정</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:94.44mm;height:3.53mm;width:134mm;"><span class="hrt cs6">※ 부트스트래핑 샘플링은 전체 데이터에서 일부 데이터의 중첩을 허용하여 랜</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:100.79mm;height:3.53mm;width:134mm;"><span class="hrt cs6">덤하게 샘플을 뽑는 방법이다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:108.90mm;height:3.53mm;width:134mm;"></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:117.02mm;height:3.53mm;width:134mm;"><span class="hrt cs6">따라서 랜덤포레스트는 여러 개의 결정트리를 만들고 그 결정트리들을 부트스</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:123.37mm;height:3.53mm;width:134mm;"><span class="hrt cs6">트랩 방식으로 복원 추출하여 여러 개의 샘플을 만들고 만든 샘플들 각각의 예측</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:129.72mm;height:3.53mm;width:134mm;"><span class="hrt cs6">값을 알아보고 그 값들의 결과를 집계해서 최종 예측값을 결정하는 모델인 것이</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:136.07mm;height:3.53mm;width:134mm;"><span class="hrt cs6">다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:144.18mm;height:3.53mm;width:134mm;"><span class="hrt cs6">이러한 RandomForest는 쉽고 직관적이며 비교적 빠른 수행속도를 가지고 다양</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:150.53mm;height:3.53mm;width:134mm;"><span class="hrt cs6">한 분야에서 좋은 성능을 나타낸다는 장점이 있지만 수많은 의사결정나무를 만들</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:156.88mm;height:3.53mm;width:134mm;"><span class="hrt cs6">어야하기 때문에 학습 시간과 연산이 많이 든다는 단점 또한 존재한다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:4.10mm;white-space:nowrap;left:0mm;top:164.92mm;height:4.94mm;width:134mm;"></div><div class="hls ps32" style="padding-left:3.53mm;line-height:4.10mm;white-space:nowrap;left:0mm;top:175.58mm;height:4.94mm;width:134mm;"></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:30mm;top:33.53mm;width:2.03mm;height:3.53mm;"><span class="hrt cs0">9</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hfS" style="left:0mm;top:171.45mm;width:50mm;height:0.11mm;"><svg class="hs" viewBox="-0.12 -0.12 50.23 0.35" style="left:-0.12mm;top:-0.12mm;width:50.23mm;height:0.35mm;left:0;top:0;"><path d="M0,0.06 L50,0.06" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path></svg></div><div class="hcD" style="left:0mm;top:173.51mm;"><div class="hcI"><div class="hls ps1" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:-0.16mm;height:3.17mm;width:134mm;"><div class="haN" style="left:0mm;top:0mm;width:2.92mm;height:3.17mm;"><span class="hrt cs3">7)</span></div><span class="hrt cs3">&nbsp;</span><span class="hrt cs29">https://velog.io/@sset2323/04-05.-GBMGradient-Boosting-Machine</span></div></div></div><div class="hcD" style="left:0mm;top:177.68mm;"><div class="hcI"><div class="hls ps1" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:-0.16mm;height:3.17mm;width:134mm;"><div class="haN" style="left:0mm;top:0mm;width:2.92mm;height:3.17mm;"><span class="hrt cs3">8)</span></div><span class="hrt cs3">&nbsp;</span><span class="hrt cs29">경사 하강법은 1차 근삿값 발견용 최적화 알고리즘이다. 기본 개념은 함수의 기울기(경사)를 구하</span></div><div class="hls ps1" style="padding-left:4.62mm;line-height:2.48mm;white-space:nowrap;left:0mm;top:3.98mm;height:3.17mm;width:134mm;"><span class="hrt cs29">고 경사의 반대 방향으로 계속 이동시켜 극값에 이를 때까지 반복시키는 것이다.</span></div></div></div><div class="hcI"><div class="hls ps32" style="padding-left:3.53mm;line-height:4.10mm;white-space:nowrap;left:0mm;top:1.52mm;height:4.94mm;width:134mm;"><span class="hrt cs7">3.2 그레디언트 부스팅 (Gradient Boosting)</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:12.24mm;height:3.53mm;width:134mm;"><span class="hrt cs6">부스팅 알고리즘은 여러 개의 모델을 순차적으로 학습, 예측하며 잘못 예측한&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:18.59mm;height:3.53mm;width:134mm;"><span class="hrt cs6">데이터에 가중치 부여를 통해 오류를 개선해 나가면서 학습하는 방식이다. 그리고&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:24.94mm;height:3.53mm;width:134mm;"><span class="hrt cs6">이러한 부스팅의 대표적인 예시로는 그래디언트 부스트(Gradient Boost)가 있다.</span></div><div class="hls ps33" style="padding-left:3.53mm;line-height:60.47mm;white-space:nowrap;left:0mm;top:33.23mm;height:60.47mm;width:134mm;"><div class="hsG" style="width:126.70mm;height:54.95mm;display:inline-block;position:relative;vertical-align:middle;"><div class="hsR" style="left:0mm;margin-right:0mm;width:126.70mm;height:54.95mm;background-repeat:no-repeat;background-image:url('web_test_hd7.png');"></div><div class="hcD" style="left:0mm;top:56.94mm;width:126.70mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:126.69mm;"><span class="hrt cs1">[그림&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:2.10mm;height:3.53mm;"><span class="hrt cs1">5</span></div><span class="hrt cs1">]</span></div></div></div></div></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:98.11mm;height:3.53mm;width:134mm;"><span class="hrt cs6">위 [그림 5]</span><span class="hfN" style="top:-1.76mm;"><span class="hrt cs6" style="font-size:5pt;top:-1pt;">7)</span></span><span class="hrt cs6">처럼 "+"와 "-"로 구성된 피쳐 dataset이 있다면 step 1에서 일</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:104.46mm;height:3.53mm;width:134mm;"><span class="hrt cs6">단 분류를 하고 step 2에선 잘못 분류한 data에 가중치를 부여한다. 이 과정을&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:110.81mm;height:3.53mm;width:134mm;"><span class="hrt cs6">step 5까지 반복하면 총 3개의 분류기(분류기준 1,2,3)를 통해 분류를 하게 되고&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:117.16mm;height:3.53mm;width:134mm;"><span class="hrt cs6">1개의 분류기를 사용할 때보다 정확도가 높아진 것을 알 수 있다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:125.27mm;height:3.53mm;width:134mm;"><span class="hrt cs6">GBC(Gradient Boosting Classifier)는 이러한 알고리즘을 가지는데 가중치 업</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:131.62mm;height:3.53mm;width:134mm;"><span class="hrt cs6">데이트를 경사하강법(Gradient Descent)</span><span class="hfN" style="top:-1.76mm;"><span class="hrt cs6" style="font-size:5pt;top:-1pt;">8)</span></span><span class="hrt cs6">을 이용하는 것이 가장 큰 특징이다. 오</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:137.97mm;height:3.53mm;width:134mm;"><span class="hrt cs6">류값은 (실제값-예측값)이 되고 이 오류 식을 최소화하는 방향으로 반복적으로 가</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:144.32mm;height:3.53mm;width:134mm;"><span class="hrt cs6">중치를 업데이트한다. gradient descent는 말 그대로 보면 &nbsp;gradient &nbsp;= 기울기,&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:150.67mm;height:3.53mm;width:134mm;"><span class="hrt cs6">descent &nbsp;= 하강이라는 두 단어가 합쳐진 단어이다. 따라서 gradient descent는&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:157.02mm;height:3.53mm;width:134mm;"><span class="hrt cs6">주어진 함수에서 극소점을 찾기 위해 기울기가 최소가 되는 지점을 찾아가는 방</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:163.37mm;height:3.53mm;width:134mm;"><span class="hrt cs6">식이다.</span></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:159.94mm;top:33.53mm;width:4.06mm;height:3.53mm;"><span class="hrt cs0">10</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hfS" style="left:0mm;top:179.76mm;width:50mm;height:0.11mm;"><svg class="hs" viewBox="-0.12 -0.12 50.23 0.35" style="left:-0.12mm;top:-0.12mm;width:50.23mm;height:0.35mm;left:0;top:0;"><path d="M0,0.06 L50,0.06" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path></svg></div><div class="hcD" style="left:0mm;top:181.82mm;"><div class="hcI"><div class="hls ps1" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:-0.16mm;height:3.17mm;width:134mm;"><div class="haN" style="left:0mm;top:0mm;width:2.92mm;height:3.17mm;"><span class="hrt cs3">9)</span></div><span class="hrt cs3">&nbsp;</span><span class="hrt cs29">https://dailyheumsi.tistory.com/136</span></div></div></div><div class="hcI"><div class="hls ps32" style="padding-left:3.53mm;line-height:4.10mm;white-space:nowrap;left:0mm;top:1.52mm;height:4.94mm;width:134mm;"><span class="hrt cs7">3.3 CatBoost</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:12.24mm;height:3.53mm;width:134mm;"><span class="hrt cs6">CatBoost는 이름에서부터 알 수 있듯이 Boosting(부스팅) 기반의 모델이다.&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:18.59mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Boosting은 Gradient Boosting Classifier에서도 설명했듯이 여러 개의 모델을&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:24.94mm;height:3.53mm;width:134mm;"><span class="hrt cs6">순차적으로 학습, 예측하며 잘못 예측한 데이터에 가중치 부여를 통해 오류를 개</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:31.29mm;height:3.53mm;width:134mm;"><span class="hrt cs6">선해 나가면서 학습하는 방식이다</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:39.41mm;height:3.53mm;width:134mm;"><span class="hrt cs6">대부분 boosting 기반 알고리즘들은 데이터를 가지고 모델이 학습을 한 후 예</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:45.76mm;height:3.53mm;width:134mm;"><span class="hrt cs6">측을 하기 위해서 범주형으로 된 모든 데이터를 일련의 과정을 통해 숫자형으로&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:52.11mm;height:3.53mm;width:134mm;"><span class="hrt cs6">바꿔준다. 반면 CatBoost에서는 다른 boosting 기반 알고리즘과 달리 범주형 변</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:58.46mm;height:3.53mm;width:134mm;"><span class="hrt cs6">수를 특별하게 처리한다. 범주형 변수를 One-hot Encoding, Label Encoding 등&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:64.81mm;height:3.53mm;width:134mm;"><span class="hrt cs6">범주형을 숫자형으로 바꿔주는 encoding 작업을 하지 않고도 범주형 변수를 그대</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:71.16mm;height:3.53mm;width:134mm;"><span class="hrt cs6">로 모델에 넣어주면 CatBoost가 알아서 Orderd Target Encoding를 진행하여 모</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:77.51mm;height:3.53mm;width:134mm;"><span class="hrt cs6">델이 학습을 하고 예측까지 할 수 있다는 장점이 있다. 이때 Orderd Target&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:83.86mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Encoding은 범주형 변수를 인코딩시킬 때, target의 값을 고려한 숫자를 인코딩</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:90.21mm;height:3.53mm;width:134mm;"><span class="hrt cs6">하는데 이는 현재 데이터의 인코딩하기 전의 데이터들의 타겟값을 이용하여 인코</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:96.56mm;height:3.53mm;width:134mm;"><span class="hrt cs6">딩을 해서 데이터의 누수와 오버피팅을 막을 수 있는 방식이다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:104.67mm;height:3.53mm;width:134mm;"><span class="hrt cs6">또한 CatBoost에서는 일반적인 Boosting 대신 Orderd Boosting을 사용한다.&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:111.02mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Ordered Boosting에서는 데이터셋이 무작위로 섞이고, 각 학습 단계에서는 이전&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:117.37mm;height:3.53mm;width:134mm;"><span class="hrt cs6">단계까지의 데이터만 사용하여 모델을 훈련시킨다. 그리고 각 학습 단계에서의 오</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:123.72mm;height:3.53mm;width:134mm;"><span class="hrt cs6">차는 다음 단계에서 사용되는 데이터에 영향을 미친다. 아래 [그림 6]의</span><span class="hfN" style="top:-1.76mm;"><span class="hrt cs6" style="font-size:5pt;top:-1pt;">9)</span></span><span class="hrt cs6">&nbsp;예시를&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:130.07mm;height:3.53mm;width:134mm;"><span class="hrt cs6">통해서 더 자세하게 설명해보겠다.</span></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:30mm;top:33.53mm;width:4.06mm;height:3.53mm;"><span class="hrt cs0">11</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hcI"><div class="hls ps33" style="padding-left:3.53mm;line-height:44.35mm;white-space:nowrap;left:0mm;top:1.76mm;height:44.35mm;width:134mm;"><div class="hsG" style="width:63.61mm;height:38.83mm;display:inline-block;position:relative;vertical-align:middle;"><div class="hsR" style="left:0mm;margin-right:0mm;width:63.61mm;height:38.83mm;background-repeat:no-repeat;background-image:url('web_test_hd8.png');"></div><div class="hcD" style="left:0mm;top:40.83mm;width:63.61mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:63.61mm;"><span class="hrt cs1">[그림&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:2.10mm;height:3.53mm;"><span class="hrt cs1">6</span></div><span class="hrt cs1">]</span></div></div></div></div></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:50.53mm;height:3.53mm;width:134mm;"><span class="hrt cs6">기존 부스팅 기법은 모든 datapoint(x1-x10)까지의 잔차를 일괄 계산한다. 반</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:56.88mm;height:3.53mm;width:134mm;"><span class="hrt cs6">면, Ordered Boosting의 과정은 다음과 같다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:64.99mm;height:3.53mm;width:134mm;"><span class="hrt cs6">1. 먼저 x1의 잔차만 계산하고, 이를 기반으로 모델을 만든다. 그 후, x2의 잔</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:71.34mm;height:3.53mm;width:134mm;"><span class="hrt cs6">차를 이 모델로 예측한다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:79.46mm;height:3.53mm;width:134mm;"><span class="hrt cs6">2. x1,x2의 잔차를 가지고 모델을 만든다. 이를 기반으로 x3,x4의 잔차를 모델</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:85.81mm;height:3.53mm;width:134mm;"><span class="hrt cs6">로 예측한다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:93.92mm;height:3.53mm;width:134mm;"><span class="hrt cs6">3. x1,x2,x3,x4의 잔차를 가지고 모델을 만든다. 이를 기반으로 x5,x6,x7,x8의&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:100.27mm;height:3.53mm;width:134mm;"><span class="hrt cs6">잔차를 모델로 예측한다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:108.38mm;height:3.53mm;width:134mm;"><span class="hrt cs6">4. 이 과정을 반복한다.</span></div><div class="hls ps37" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:116.50mm;height:3.53mm;width:134mm;"><span class="hrt cs6">즉 순서(order)에 따라 모델을 학습시키고 순차적으로 잔차를 계산하는 과정을&nbsp;</span></div><div class="hls ps37" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:122.14mm;height:3.53mm;width:134mm;"><span class="hrt cs6">반복하는 것이다.</span></div><div class="hls ps37" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:129.55mm;height:3.53mm;width:134mm;"></div><div class="hls ps32" style="padding-left:3.53mm;line-height:4.10mm;white-space:nowrap;left:0mm;top:136.89mm;height:4.94mm;width:134mm;"><span class="hrt cs7">3.4 &nbsp;XGBoost (</span><span class="hrt cs26">eXtreme Gradient Boosting)</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:147.61mm;height:3.53mm;width:134mm;"><span class="hrt cs6">XGBoost는 "Extreme Gradient Boosting"의 약자이다. 이는 여러 개의&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:153.96mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Decision Tree를 조합해서 사용하는 앙상블 알고리즘으로 기존의 그래디언트 부</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:160.31mm;height:3.53mm;width:134mm;"><span class="hrt cs6">스팅 알고리즘의 계산 속도와 모델 성능을 효과적으로 향상시킨 것이 특징이다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:168.43mm;height:3.53mm;width:134mm;"><span class="hrt cs6">대부분의 트리 기반 알고리즘은 크게 Level-wise 트리 분할과 Leaf-wise 트리&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:174.78mm;height:3.53mm;width:134mm;"><span class="hrt cs6">분할로 나뉘게 된다. 이 중에서 XGBoost는 Level-wise 트리 분할 알고리즘을 사</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:181.13mm;height:3.53mm;width:134mm;"><span class="hrt cs6">용한다.</span></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:159.94mm;top:33.53mm;width:4.06mm;height:3.53mm;"><span class="hrt cs0">12</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hcI"><div class="hls ps33" style="padding-left:3.53mm;line-height:50.04mm;white-space:nowrap;left:0mm;top:1.76mm;height:50.04mm;width:134mm;"><div class="hsG" style="width:83.92mm;height:44.52mm;display:inline-block;position:relative;vertical-align:middle;"><div class="hsR" style="left:0mm;margin-right:0mm;width:83.92mm;height:44.52mm;background-repeat:no-repeat;background-image:url('web_test_hd9.png');"></div><div class="hcD" style="left:0mm;top:46.51mm;width:83.92mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:83.92mm;"><span class="hrt cs1">[그림&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:2.10mm;height:3.53mm;"><span class="hrt cs1">7</span></div><span class="hrt cs1">]</span></div></div></div></div></div><div class="hls ps39" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:56.22mm;height:3.53mm;width:134mm;"><span class="hrt cs6">일반적인 Gradiant Boosting 모델의 트리 분할은 균형 트리 분할 방법(Level&nbsp;</span></div><div class="hls ps39" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:61.69mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Wise)을 사용한다. 균형 트리 분할은 트리의 깊이를 효과적으로 줄일 수 있다. 즉&nbsp;</span></div><div class="hls ps39" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:67.17mm;height:3.53mm;width:134mm;"><span class="hrt cs6">최대한 균형 잡힌 트리를 유지하면서 분할하기 때문에 트리의 깊이가 최소화될&nbsp;</span></div><div class="hls ps39" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:72.64mm;height:3.53mm;width:134mm;"><span class="hrt cs6">수 있다. 균형 잡힌 트리를 생성하는 이유는 overfitting에 보다 더 강한 구조를&nbsp;</span></div><div class="hls ps39" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:78.12mm;height:3.53mm;width:134mm;"><span class="hrt cs6">가질 수 있다고 알려져 있기 때문이다.&nbsp;</span><span class="hrt cs25">XGBoost의 핵심적인 특징</span><span class="hrt cs6">은 다음과 같다.</span></div><div class="hls ps39" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:85.35mm;height:3.53mm;width:134mm;"></div><div class="hls ps39" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:92.59mm;height:3.53mm;width:134mm;"><span class="hrt cs6">1. 기존 Gradient Boosting 알고리즘은 트리의 깊이가 일정 수준 이상이면 가</span></div><div class="hls ps39" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:98.07mm;height:3.53mm;width:134mm;"><span class="hrt cs6">지치기를 하지 않고 최대한 깊게 트리를 만든 반면 XGBoost는 가지치기를 통해&nbsp;</span></div><div class="hls ps39" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:103.54mm;height:3.53mm;width:134mm;"><span class="hrt cs6">더욱 간결하면서 예측 성능이 우수한 트리 구조를 만들 수 있다. 여기서 말하는&nbsp;</span></div><div class="hls ps39" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:109.02mm;height:3.53mm;width:134mm;"><span class="hrt cs6">가지치기는 트리의 깊이가 깊어질수록 과적합의 위험성이 높아지는데 이때 불필</span></div><div class="hls ps39" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:114.49mm;height:3.53mm;width:134mm;"><span class="hrt cs6">요한 마디를 제거하는 과정이 바로 가지치기이다.</span></div><div class="hls ps39" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:121.73mm;height:3.53mm;width:134mm;"><span class="hrt cs6">2. XGBoost는 과적합을 방지하기 위해 규제(Regularization)를 적용할 수 있다.&nbsp;</span></div><div class="hls ps39" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:127.21mm;height:3.53mm;width:134mm;"><span class="hrt cs6">규제는 L1, L2 규제를 지원한다. 이때 L1규제는 모델의 가중치에 대한 절대값의&nbsp;</span></div><div class="hls ps39" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:132.68mm;height:3.53mm;width:134mm;"><span class="hrt cs6">합을 계산하여 이를 손실 함수에 추가함으로써 모델의 가중치를 0으로 만들어 불</span></div><div class="hls ps39" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:138.16mm;height:3.53mm;width:134mm;"><span class="hrt cs6">필요한 특성을 제거하고 모델을 간단하게 만드는 효과가 있고, L2규제는 모델의&nbsp;</span></div><div class="hls ps39" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:143.63mm;height:3.53mm;width:134mm;"><span class="hrt cs6">가중치에 대한 제곱의 합을 계산하여 이를 손실 함수에 추가하여 모델의 모든 가</span></div><div class="hls ps39" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:149.11mm;height:3.53mm;width:134mm;"><span class="hrt cs6">중치를 작게 만드는 효과가 있어, 모델이 학습 데이터에 너무 민감하게 반응하는&nbsp;</span></div><div class="hls ps39" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:154.58mm;height:3.53mm;width:134mm;"><span class="hrt cs6">것을 방지할 수 있다.</span></div><div class="hls ps39" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:161.82mm;height:3.53mm;width:134mm;"></div><div class="hls ps38" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:169.06mm;height:3.53mm;width:134mm;"><span class="hrt cs6">이러한 특징으로 인해서 XGBoost는&nbsp;</span><span class="hrt cs25">과적합을 방지하는데에 적합</span><span class="hrt cs6">해서 이를 통</span></div><div class="hls ps38" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:175.06mm;height:3.53mm;width:134mm;"><span class="hrt cs6">해 모델의 예측 성능이 좋다는 장점이 있는 반면 좋은 성능을 보이기 위해서는&nbsp;</span></div><div class="hls ps38" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:181.06mm;height:3.53mm;width:134mm;"><span class="hrt cs6">충분히 많은 데이터가 있어야하며 이는 다른 말로 하면 충분치 않은 데이터에서</span></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:30mm;top:33.53mm;width:4.06mm;height:3.53mm;"><span class="hrt cs0">13</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hfS" style="left:0mm;top:179.76mm;width:50mm;height:0.11mm;"><svg class="hs" viewBox="-0.12 -0.12 50.23 0.35" style="left:-0.12mm;top:-0.12mm;width:50.23mm;height:0.35mm;left:0;top:0;"><path d="M0,0.06 L50,0.06" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path></svg></div><div class="hcD" style="left:0mm;top:181.82mm;"><div class="hcI"><div class="hls ps1" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:-0.16mm;height:3.17mm;width:134mm;"><div class="haN" style="left:0mm;top:0mm;width:4.71mm;height:3.17mm;"><span class="hrt cs3">10)</span></div><span class="hrt cs3">&nbsp;</span><span class="hrt cs29">https://m.blog.naver.com/winddori2002/221931868686</span></div></div></div><div class="hcI"><div class="hls ps38" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:134mm;"><span class="hrt cs6">는 과적합을 보일 수 있다는 이야기이다.</span></div><div class="hls ps38" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:7.58mm;height:3.53mm;width:134mm;"></div><div class="hls ps38" style="padding-left:3.53mm;line-height:4.10mm;white-space:nowrap;left:0mm;top:15.28mm;height:4.94mm;width:134mm;"><span class="hrt cs7">3.5 LightGBM</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:25.51mm;height:3.53mm;width:134mm;"><span class="hrt cs6">LightGBM은 XGBoost와 마찬가지로 Decision Tree를 조합해서 사용하는</span><span class="hrt cs25">&nbsp;앙상</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:31.86mm;height:3.53mm;width:134mm;"><span class="hrt cs25">블 알고리즘</span><span class="hrt cs6">이다. XGBoost의 경우 매우 뛰어난 부스팅 알고리즘이지만 여전히 학</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:38.21mm;height:3.53mm;width:134mm;"><span class="hrt cs6">습 시간이 오래 걸린다는 단점이 있는 반면 LightGBM의 가장 큰 장점은&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:44.56mm;height:3.53mm;width:134mm;"><span class="hrt cs6">XGBoost보다 학습에 걸리는 시간이 훨씬 적고 메모리 사용량도 상대적으로 적다.&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:50.91mm;height:3.53mm;width:134mm;"><span class="hrt cs6">LighGBM과 XGBoost의 예측 성능은 별다른 차이가 존재하지 않는다. 또한 기능</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:57.26mm;height:3.53mm;width:134mm;"><span class="hrt cs6">상의 다양성은 LightGBM이 약간 더 많다. 따라서 LightGBM은 XGBoost의 장점</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:63.61mm;height:3.53mm;width:134mm;"><span class="hrt cs6">은 계승하고 단점은 보완하는 방식으로 개발되었다. 그럼에도 불구하고&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:69.96mm;height:3.53mm;width:134mm;"><span class="hrt cs6">LightGBM의 한 가지 단점으로는 적은 데이터 셋에 적용할 경우 과적합이 발생하</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:76.31mm;height:3.53mm;width:134mm;"><span class="hrt cs6">기 쉽다는 것이다.</span></div><div class="hls ps33" style="padding-left:3.53mm;line-height:45.62mm;white-space:nowrap;left:0mm;top:84.60mm;height:45.62mm;width:134mm;"><div class="hsG" style="width:83.12mm;height:40.10mm;display:inline-block;position:relative;vertical-align:middle;"><div class="hsR" style="left:0mm;margin-right:0mm;width:83.12mm;height:40.10mm;background-repeat:no-repeat;background-image:url('web_test_hda.png');"></div><div class="hcD" style="left:0mm;top:42.09mm;width:83.12mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:83.11mm;"><span class="hrt cs1">[그림&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:2.10mm;height:3.53mm;"><span class="hrt cs1">8</span></div><span class="hrt cs1">]</span></div></div></div></div><span class="hfN" style="top:-1.76mm;"><span class="hrt cs6" style="font-size:5pt;top:-1pt;">10)</span></span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:134.63mm;height:3.53mm;width:134mm;"><span class="hrt cs6">LightGBM은 일반 GBM 계열의 트리 분할 방법과 달리 리프 중심 트리 분할</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:140.98mm;height:3.53mm;width:134mm;"><span class="hrt cs6">(Leaf Wise) 방식을 사용한다. LightGBM의 리프 중심 트리 분할 방식은 트리의&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:147.33mm;height:3.53mm;width:134mm;"><span class="hrt cs6">균형을 맞추지 않고, 최대 손실 값을 가지는 리프 노드를 지속적으로 분할하면서&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:153.68mm;height:3.53mm;width:134mm;"><span class="hrt cs6">트리의 깊이가 깊어지고 비대칭적인 규칙 트리가 생성된다. 하지만 이렇게 최대&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:160.03mm;height:3.53mm;width:134mm;"><span class="hrt cs6">손실값을 가지는 리프 노드를 지속적으로 분할해 생성된 규칙 트리는 학습을 반</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:166.38mm;height:3.53mm;width:134mm;"><span class="hrt cs6">복할수록 결국은 균형 트리 분할 방식보다 예측 오류 손실을 최소화 할 수 있다.</span></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:159.94mm;top:33.53mm;width:4.06mm;height:3.53mm;"><span class="hrt cs0">14</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hcI"><div class="hls ps32" style="padding-left:3.53mm;line-height:4.10mm;white-space:nowrap;left:0mm;top:1.52mm;height:4.94mm;width:134mm;"><span class="hrt cs28">4. 본론</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:12.24mm;height:3.53mm;width:134mm;"><span class="hrt cs6">전체적인 연구 흐름은 다음 [그림 9]와 같다.</span></div><div class="hls ps33" style="padding-left:3.53mm;line-height:93.11mm;white-space:nowrap;left:0mm;top:20.53mm;height:93.11mm;width:134mm;"><div class="hsG" style="width:75.35mm;height:87.58mm;display:inline-block;position:relative;vertical-align:middle;"><div class="hsR" style="left:0mm;margin-right:0mm;width:75.35mm;height:87.58mm;background-repeat:no-repeat;background-image:url('web_test_hdb.png');"></div><div class="hcD" style="left:0mm;top:89.58mm;width:75.35mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:75.35mm;"><span class="hrt cs1">[그림&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:2.10mm;height:3.53mm;"><span class="hrt cs1">9</span></div><span class="hrt cs1">]</span></div></div></div></div></div><div class="hls ps32" style="padding-left:3.53mm;line-height:4.10mm;white-space:nowrap;left:0mm;top:117.98mm;height:4.94mm;width:134mm;"></div><div class="hls ps32" style="padding-left:3.53mm;line-height:4.10mm;white-space:nowrap;left:0mm;top:128.63mm;height:4.94mm;width:134mm;"><span class="hrt cs26">4.1 분석 데이터 구성</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:139.36mm;height:3.53mm;width:134mm;"><span class="hrt cs6">&nbsp;데이콘이 주최하는 월간 데이콘 천체 유형 분류 대회에서는 fiberID, psfMag,&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:145.71mm;height:3.53mm;width:134mm;"><span class="hrt cs6">fiberMag 등과 같은 천체를 관측하여 측정된 21개의 데이터를 포함하는 train(훈</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:152.06mm;height:3.53mm;width:134mm;"><span class="hrt cs6">련)데이터셋과 test(시험)데이터셋을 제공한다.</span></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:30mm;top:33.53mm;width:4.06mm;height:3.53mm;"><span class="hrt cs0">15</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hcI"><div class="hls ps33" style="padding-left:3.53mm;line-height:71.50mm;white-space:nowrap;left:0mm;top:1.76mm;height:71.50mm;width:134mm;"><div class="hsG" style="width:126.94mm;height:65.97mm;display:inline-block;position:relative;vertical-align:middle;"><div class="hsR" style="left:0mm;margin-right:0mm;width:126.94mm;height:65.97mm;background-repeat:no-repeat;background-image:url('web_test_hdc.png');"></div><div class="hcD" style="left:0mm;top:67.97mm;width:126.94mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:126.94mm;"><span class="hrt cs1">[그림&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:4.21mm;height:3.53mm;"><span class="hrt cs1">10</span></div><span class="hrt cs1">]</span></div></div></div></div></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:77.67mm;height:3.53mm;width:134mm;"><span class="hrt cs6">여기서 train 데이터셋은 다섯 종류의 천체 관측 데이터와 19개의 천체 유형을&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:84.02mm;height:3.53mm;width:134mm;"><span class="hrt cs6">포함하여 총 199991건의 천체 데이터를 제시한다. test 데이터셋은 train 데이터</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:90.37mm;height:3.53mm;width:134mm;"><span class="hrt cs6">셋에서 천체 유형을 제외한 10009건의 데이터를 제시한다. 본 연구에서는 train&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:96.72mm;height:3.53mm;width:134mm;"><span class="hrt cs6">데이터셋을 훈련한 모델에서 찾은 알고리즘을 train 데이터셋에 적용하여 천체&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:103.07mm;height:3.53mm;width:134mm;"><span class="hrt cs6">10009개의 유형을 분류, 예측하는 것을 목표로 한다.</span></div><div class="hls ps33" style="padding-left:3.53mm;line-height:32.11mm;white-space:nowrap;left:0mm;top:111.36mm;height:32.11mm;width:134mm;"><div class="hsG" style="width:160.93mm;height:26.58mm;display:inline-block;position:relative;vertical-align:middle;"><div class="hsR" style="left:0mm;margin-right:0mm;width:160.93mm;height:26.58mm;background-repeat:no-repeat;background-image:url('web_test_hdd.png');"></div><div class="hcD" style="left:0mm;top:28.58mm;width:160.93mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:160.92mm;"><span class="hrt cs1">[그림&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:4.21mm;height:3.53mm;"><span class="hrt cs1">11</span></div><span class="hrt cs1">]</span></div></div></div></div></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:147.88mm;height:3.53mm;width:134mm;"><span class="hrt cs6">본 대회에서 제공하는 train 데이터는 [그림 11]에서 보는 바와 같이 id와&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:154.23mm;height:3.53mm;width:134mm;"><span class="hrt cs6">type, 그리고 21개의 feature들로 이루어져 있다. ‘id’는 데이터셋에서 지정한 천</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:160.58mm;height:3.53mm;width:134mm;"><span class="hrt cs6">체들의 고유 번호를 의미한다. ‘type’는 문자열 데이터로 우리가 분류하고자 하는&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:166.93mm;height:3.53mm;width:134mm;"><span class="hrt cs6">천체의 유형을 의미한다. 이는 타겟변수로, train 데이터에서만 존재하고 test 데</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:173.28mm;height:3.53mm;width:134mm;"><span class="hrt cs6">이터에는 존재하지 않는다.</span></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:159.94mm;top:33.53mm;width:4.06mm;height:3.53mm;"><span class="hrt cs0">16</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hcI"><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:1.59mm;height:3.53mm;width:134mm;"><span class="hrt cs6">다음의 21개의 feature들은 예측변수이다. ‘fiberID’는 관측에 사용된 광섬유의&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:7.94mm;height:3.53mm;width:134mm;"><span class="hrt cs6">구분자로, 천체의 위치를 정확하게 파악하기 위해 사용된 광섬유를 구분하는 식별</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:14.29mm;height:3.53mm;width:134mm;"><span class="hrt cs6">자이다. ‘psfMag’는 point spread function magnitudes의 약자로, 먼 천체를 한&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:20.64mm;height:3.53mm;width:134mm;"><span class="hrt cs6">점으로 가정하여 측정한 빛의 밝기를 의미한다. ‘fiberMag’는 fiber magnitudes의&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:26.99mm;height:3.53mm;width:134mm;"><span class="hrt cs6">약자로, 3인치 지름의 광섬유를 사용하여 광스펙트럼을 측정하는데 이때 나타난&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:33.34mm;height:3.53mm;width:134mm;"><span class="hrt cs6">천체의 밝기를 나타낸다. ‘petroMag’는 Petrosian Magnitudes의 약자이다. 은하</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:39.69mm;height:3.53mm;width:134mm;"><span class="hrt cs6">처럼 뚜렷한 표면이 없는 천체에서는 빛의 밝기를 측정하기 어려운데, 이때 천체</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:46.04mm;height:3.53mm;width:134mm;"><span class="hrt cs6">의 위치와 거리에 상관없이 빛의 밝기를 비교하기 위한 수치이다 ‘modelMag’는&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:52.39mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Model magnitudes로, 천체 중심으로부터 특정 거리의 밝기이다. fiberID를 제외</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:58.74mm;height:3.53mm;width:134mm;"><span class="hrt cs6">한 모든 feature에는 u, g, r, i, z라는 알파벳이 존재하는데 해당 알파벳은 파장대</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:65.09mm;height:3.53mm;width:134mm;"><span class="hrt cs6">를 의미하며 구체적인 의미는 다음의 [표 1]에 따른다.</span></div><div class="hls ps33" style="padding-left:3.53mm;line-height:41.99mm;white-space:nowrap;left:0mm;top:73.38mm;height:41.99mm;width:134mm;"><div class="htG" style="left:0mm;width:124.04mm;top:0mm;height:41.99mm;display:inline-block;position:relative;vertical-align:middle;"><div class="htb" style="left:1mm;width:122.04mm;top:1mm;height:33.47mm;"><svg class="hs" viewBox="-2.50 -2.50 127.04 38.47" style="left:-2.50mm;top:-2.50mm;width:127.04mm;height:38.47mm;"><defs><pattern id="w_150" width="10" height="10" patternUnits="userSpaceOnUse"><rect width="10" height="10" fill="rgb(217,217,217)"/></pattern></defs><path fill="url(#w_150)" d="M0,0L26.13,0L26.13,5.58L0,5.58L0,0Z "></path><path fill="url(#w_150)" d="M26.13,0L62.36,0L62.36,5.58L26.13,5.58L26.13,0Z "></path><path fill="url(#w_150)" d="M62.36,0L122.04,0L122.04,5.58L62.36,5.58L62.36,0Z "></path><path d="M0,0 L0,33.47" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M26.13,0 L26.13,33.47" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M62.36,0 L62.36,33.47" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M122.04,0 L122.04,33.47" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,0 L122.04,0" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,5.58 L122.04,5.58" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,11.17 L122.04,11.17" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,16.75 L122.04,16.75" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,22.34 L122.04,22.34" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,27.92 L122.04,27.92" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,33.47 L122.04,33.47" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M122.04,0 L122.04,33.47" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,0 L0,33.47" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,33.47 L122.04,33.47" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,0 L122.04,0" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path></svg><div class="hce" style="left:0mm;top:0mm;width:26.13mm;height:5.58mm;"><div class="hcD" style="left:2mm;top:1mm;"><div class="hcI" style="top:0.03mm;"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:22.13mm;"><span class="hrt cs27">Value</span></div></div></div></div><div class="hce" style="left:26.13mm;top:0mm;width:36.23mm;height:5.58mm;"><div class="hcD" style="left:2mm;top:1mm;"><div class="hcI" style="top:0.03mm;"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:32.23mm;"><span class="hrt cs27">Definition</span></div></div></div></div><div class="hce" style="left:62.36mm;top:0mm;width:59.68mm;height:5.58mm;"><div class="hcD" style="left:2mm;top:1mm;"><div class="hcI" style="top:0.03mm;"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:55.68mm;"><span class="hrt cs27">뜻</span></div></div></div></div><div class="hce" style="left:0mm;top:5.58mm;width:26.13mm;height:5.58mm;"><div class="hcD" style="left:2mm;top:1mm;"><div class="hcI" style="top:0.03mm;"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:22.13mm;"><span class="hrt cs27">u</span></div></div></div></div><div class="hce" style="left:26.13mm;top:5.58mm;width:36.23mm;height:5.58mm;"><div class="hcD" style="left:2mm;top:1mm;"><div class="hcI" style="top:0.03mm;"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:32.23mm;"><span class="hrt cs27">Ultraviolet</span></div></div></div></div><div class="hce" style="left:62.36mm;top:5.58mm;width:59.68mm;height:5.58mm;"><div class="hcD" style="left:2mm;top:1mm;"><div class="hcI" style="top:0.03mm;"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:55.68mm;"><span class="hrt cs27">자외선</span></div></div></div></div><div class="hce" style="left:0mm;top:11.17mm;width:26.13mm;height:5.58mm;"><div class="hcD" style="left:2mm;top:1mm;"><div class="hcI" style="top:0.03mm;"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:22.13mm;"><span class="hrt cs27">g</span></div></div></div></div><div class="hce" style="left:26.13mm;top:11.17mm;width:36.23mm;height:5.58mm;"><div class="hcD" style="left:2mm;top:1mm;"><div class="hcI" style="top:0.03mm;"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:32.23mm;"><span class="hrt cs27">Green</span></div></div></div></div><div class="hce" style="left:62.36mm;top:11.17mm;width:59.68mm;height:5.58mm;"><div class="hcD" style="left:2mm;top:1mm;"><div class="hcI" style="top:0.03mm;"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:55.68mm;"><span class="hrt cs27">가시광선 중 초록색의 파장</span></div></div></div></div><div class="hce" style="left:0mm;top:16.75mm;width:26.13mm;height:5.58mm;"><div class="hcD" style="left:2mm;top:1mm;"><div class="hcI" style="top:0.03mm;"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:22.13mm;"><span class="hrt cs27">r</span></div></div></div></div><div class="hce" style="left:26.13mm;top:16.75mm;width:36.23mm;height:5.58mm;"><div class="hcD" style="left:2mm;top:1mm;"><div class="hcI" style="top:0.03mm;"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:32.23mm;"><span class="hrt cs27">Red</span></div></div></div></div><div class="hce" style="left:62.36mm;top:16.75mm;width:59.68mm;height:5.58mm;"><div class="hcD" style="left:2mm;top:1mm;"><div class="hcI" style="top:0.03mm;"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:55.68mm;"><span class="hrt cs27">가시광선 중 빨간색의 파장</span></div></div></div></div><div class="hce" style="left:0mm;top:22.34mm;width:26.13mm;height:5.58mm;"><div class="hcD" style="left:2mm;top:1mm;"><div class="hcI" style="top:0.03mm;"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:22.13mm;"><span class="hrt cs27">i</span></div></div></div></div><div class="hce" style="left:26.13mm;top:22.34mm;width:36.23mm;height:5.58mm;"><div class="hcD" style="left:2mm;top:1mm;"><div class="hcI" style="top:0.03mm;"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:32.23mm;"><span class="hrt cs27">Near Infrared</span></div></div></div></div><div class="hce" style="left:62.36mm;top:22.34mm;width:59.68mm;height:5.58mm;"><div class="hcD" style="left:2mm;top:1mm;"><div class="hcI" style="top:0.03mm;"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:55.68mm;"><span class="hrt cs27">빨간색에 가까운 적외선</span></div></div></div></div><div class="hce" style="left:0mm;top:27.92mm;width:26.13mm;height:5.55mm;"><div class="hcD" style="left:2mm;top:1mm;"><div class="hcI" style="top:0.01mm;"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:22.13mm;"><span class="hrt cs27">z</span></div></div></div></div><div class="hce" style="left:26.13mm;top:27.92mm;width:36.23mm;height:5.55mm;"><div class="hcD" style="left:2mm;top:1mm;"><div class="hcI" style="top:0.01mm;"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:32.23mm;"><span class="hrt cs27">Infrared</span></div></div></div></div><div class="hce" style="left:62.36mm;top:27.92mm;width:59.68mm;height:5.55mm;"><div class="hcD" style="left:2mm;top:1mm;"><div class="hcI" style="top:0.01mm;"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:55.68mm;"><span class="hrt cs27">적외선</span></div></div></div></div></div><div class="hcD" style="left:1mm;top:37.46mm;width:122.04mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:122.03mm;"><span class="hrt cs1">[표&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:2.10mm;height:3.53mm;"><span class="hrt cs1">1</span></div><span class="hrt cs1">]</span></div></div></div></div></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:119.78mm;height:3.53mm;width:134mm;"><span class="hrt cs6">천체 train, test 데이터에서는 결측치가 존재하지 않아 따로 처리해줄 필요가&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:126.13mm;height:3.53mm;width:134mm;"><span class="hrt cs6">없다. 그러나 천체 유형별 불균형이 매우 심하고, 이상치가 존재한다. 데이터 불균</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:132.48mm;height:3.53mm;width:134mm;"><span class="hrt cs6">형과 이상치를 처리하기 위해 데이터 전처리를 진행한다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:4.10mm;white-space:nowrap;left:0mm;top:140.52mm;height:4.94mm;width:134mm;"></div><div class="hls ps32" style="padding-left:3.53mm;line-height:4.10mm;white-space:nowrap;left:0mm;top:151.18mm;height:4.94mm;width:134mm;"><span class="hrt cs7">4.2 데이터 전처리</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:161.90mm;height:3.53mm;width:134mm;"><span class="hrt cs25">4.2.1 데이터 불균형</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:170.01mm;height:3.53mm;width:134mm;"><span class="hrt cs6">데이터의 불균형을 살펴보면 다음과 같은 [그림 12]가 그려진다.</span></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:30mm;top:33.53mm;width:4.06mm;height:3.53mm;"><span class="hrt cs0">17</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hcI"><div class="hls ps33" style="padding-left:3.53mm;line-height:84.46mm;white-space:nowrap;left:0mm;top:1.76mm;height:84.46mm;width:134mm;"><div class="hsG" style="width:106.31mm;height:78.94mm;display:inline-block;position:relative;vertical-align:middle;"><div class="hsR" style="left:0mm;margin-right:0mm;width:106.31mm;height:78.94mm;background-repeat:no-repeat;background-image:url('web_test_hde.png');"></div><div class="hcD" style="left:0mm;top:80.93mm;width:106.31mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:106.30mm;"><span class="hrt cs1">[그림&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:4.21mm;height:3.53mm;"><span class="hrt cs1">12</span></div><span class="hrt cs1">]</span></div></div></div></div></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:90.64mm;height:3.53mm;width:134mm;"><span class="hrt cs6">[그림 12]는 train 데이터셋에서 data class의 분포도를 나타낸 그래프이다. 위</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:96.99mm;height:3.53mm;width:134mm;"><span class="hrt cs6">의 그래프를 보면 ‘QSO’가 가장 많고, ‘STAR_PN’이 가장 적으며 차이가 큼을 볼&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:103.34mm;height:3.53mm;width:134mm;"><span class="hrt cs6">수 있다. 이 상태로 모델 학습을 수행할 경우 ‘QSO’로 예측했을 때 가장 좋은 정</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:109.69mm;height:3.53mm;width:134mm;"><span class="hrt cs6">답률을 보이기 때문에 한쪽으로 치우쳐져 정확한 모델을 구현하기 매우 어렵다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:117.80mm;height:3.53mm;width:134mm;"><span class="hrt cs6">이러한 데이터 불균형의 문제를 해결하는 방법으로는 보통 data augmentation</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:124.15mm;height:3.53mm;width:134mm;"><span class="hrt cs6">(데이터 증강), data sampling(데이터 재분배), class weight(가중치 부여)가 있다.&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:130.50mm;height:3.53mm;width:134mm;"><span class="hrt cs6">데이터 증강은 새로운 데이터를 수집하지 않고 모델 훈련에 사용할 수 있는 데이</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:136.85mm;height:3.53mm;width:134mm;"><span class="hrt cs6">터의 다양성을 증가시키기 위한 전략이다. 데이터 재분배는 데이터셋의 조합을 균</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:143.20mm;height:3.53mm;width:134mm;"><span class="hrt cs6">형있게 맞추어 여러개의 데이터셋을 만들어 모델 학습에 사용되는 데이터가 균등</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:149.55mm;height:3.53mm;width:134mm;"><span class="hrt cs6">하게 적용될 수 있도록 하는 방법이다. Class Weight는 해당 Class의 비율의 역</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:155.90mm;height:3.53mm;width:134mm;"><span class="hrt cs6">수 값의 가중치를 적용하여 ‘QSO’처럼 빈도수가 높은 Class가 모델에 미치는 영</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:162.25mm;height:3.53mm;width:134mm;"><span class="hrt cs6">향은 줄이고, ‘STAR_PN’처럼 빈도수가 낮은 Class가 모델에 미치는 영향을 높여&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:168.60mm;height:3.53mm;width:134mm;"><span class="hrt cs6">균등하게 만드는 방법이다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:176.71mm;height:3.53mm;width:134mm;"><span class="hrt cs6">그러나 본 연구에서는 이러한 다중 분류에서 나타날 수 있는 문제들을 해결하</span></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:159.94mm;top:33.53mm;width:4.06mm;height:3.53mm;"><span class="hrt cs0">18</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hcI"><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:134mm;"><span class="hrt cs6">기보다, 비율이 비슷한 ‘SPECTROPHOTO_STD’와 ‘REDDEN_STD’를 가지고 이</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:6.17mm;height:3.53mm;width:134mm;"><span class="hrt cs6">진 분류를 한다. 이 두 class만을 분류함으로써 불균형 문제에서 벗어나고 연구함</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:12.52mm;height:3.53mm;width:134mm;"><span class="hrt cs6">에 있어서 해석하기 쉬운 방향으로 진행하고자 한다.</span></div><div class="hls ps34" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:20.64mm;height:3.53mm;width:134mm;"><span class="hrt cs25">4.2.2 이상치 제거</span></div><div class="hls ps33" style="padding-left:3.53mm;line-height:49.09mm;white-space:nowrap;left:0mm;top:28.93mm;height:49.09mm;width:134mm;"><div class="hsG" style="width:127.83mm;height:43.56mm;display:inline-block;position:relative;vertical-align:middle;"><div class="hsR" style="left:0mm;margin-right:0mm;width:127.83mm;height:43.56mm;background-repeat:no-repeat;background-image:url('web_test_hdf.png');"></div><div class="hcD" style="left:0mm;top:45.56mm;width:127.83mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:127.82mm;"><span class="hrt cs1">[그림&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:4.21mm;height:3.53mm;"><span class="hrt cs1">13</span></div><span class="hrt cs1">]</span></div></div></div></div></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:82.43mm;height:3.53mm;width:134mm;"><span class="hrt cs6">위의 [그림 13]와 같이 train 데이터셋에는 이상치라고 생각되는 높고 낮은 수</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:88.78mm;height:3.53mm;width:134mm;"><span class="hrt cs6">치들이 존재한다. 이러한 이상치를 제거하기 위한 방법에는 여러가지가 있지만,&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:95.13mm;height:3.53mm;width:134mm;"><span class="hrt cs6">본 연구에서는 통계적 방법, 그중에서도 사분위수(Quartiles) 방법을 사용한다. 사</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:101.48mm;height:3.53mm;width:134mm;"><span class="hrt cs6">분위수 방법은 데이터 분포와 값의 크기를 이용하여 대략적인 이상치의 구간을&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:107.83mm;height:3.53mm;width:134mm;"><span class="hrt cs6">설정해준다. 구체적으로는 데이터의 분포에 따라 4등분을 하여 각 부분의 값을 각</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:114.18mm;height:3.53mm;width:134mm;"><span class="hrt cs6">각 1사분위, 2사분위, 3사분위, 4사분위로 나타낸다. 3사분위 수와 1사분위 수의&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:120.53mm;height:3.53mm;width:134mm;"><span class="hrt cs6">차로 IQR(Interquartile Range) 값을 구한 후, IQR * 1.5를 하여 그 이상 벗어난&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:126.88mm;height:3.53mm;width:134mm;"><span class="hrt cs6">값들을 이상치로 판별하고 제거한다.</span></div><div class="hls ps33" style="padding-left:3.53mm;line-height:46.80mm;white-space:nowrap;left:0mm;top:135.17mm;height:46.80mm;width:134mm;"><div class="hsG" style="width:128.71mm;height:41.27mm;display:inline-block;position:relative;vertical-align:middle;"><div class="hsR" style="left:0mm;margin-right:0mm;width:128.71mm;height:41.27mm;background-repeat:no-repeat;background-image:url('web_test_hd10.png');"></div><div class="hcD" style="left:0mm;top:43.27mm;width:128.71mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:128.71mm;"><span class="hrt cs1">[그림&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:4.21mm;height:3.53mm;"><span class="hrt cs1">14</span></div><span class="hrt cs1">]</span></div></div></div></div></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:30mm;top:33.53mm;width:4.06mm;height:3.53mm;"><span class="hrt cs0">19</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hcI"><div class="hls ps33" style="padding-left:3.53mm;line-height:91.68mm;white-space:nowrap;left:0mm;top:1.76mm;height:91.68mm;width:134mm;"><div class="htG" style="left:0mm;width:134mm;top:0mm;height:91.68mm;display:inline-block;position:relative;vertical-align:middle;"><div class="htb" style="left:1mm;width:132mm;top:1mm;height:83.16mm;"><svg class="hs" viewBox="-2.50 -2.50 137 88.16" style="left:-2.50mm;top:-2.50mm;width:137mm;height:88.16mm;"><path d="M0,0 L0,83.16" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M66,0 L66,83.16" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M132,0 L132,83.16" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,0 L132.01,0" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,4.52 L132.01,4.52" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,83.16 L132.01,83.16" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M132,0 L132,83.16" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,0 L0,83.16" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,83.16 L132.01,83.16" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,0 L132.01,0" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path></svg><div class="hce" style="left:0mm;top:0mm;width:66mm;height:4.52mm;"><div class="hcD" style="left:1.80mm;top:0.50mm;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:62.40mm;"><span class="hrt cs1">이상치 제거 전</span></div></div></div></div><div class="hce" style="left:66mm;top:0mm;width:66mm;height:4.52mm;"><div class="hcD" style="left:1.80mm;top:0.50mm;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:62.40mm;"><span class="hrt cs1">이상치 제거 후</span></div></div></div></div><div class="hce" style="left:0mm;top:4.52mm;width:66mm;height:78.63mm;"><div class="hcD" style="left:1.80mm;top:0.50mm;"><div class="hcI"><div class="hls ps13" style="line-height:77.64mm;white-space:nowrap;left:0mm;top:0mm;height:77.64mm;width:62.40mm;"><div class="hsR" style="top:0mm;left:0mm;margin-bottom:0mm;margin-right:0mm;width:53.71mm;height:77.64mm;display:inline-block;position:relative;vertical-align:middle;background-repeat:no-repeat;background-image:url('web_test_hd11.png');"></div></div></div></div></div><div class="hce" style="left:66mm;top:4.52mm;width:66mm;height:78.63mm;"><div class="hcD" style="left:1.80mm;top:0.50mm;"><div class="hcI" style="top:0.23mm;"><div class="hls ps13" style="line-height:77.19mm;white-space:nowrap;left:0mm;top:0mm;height:77.19mm;width:62.40mm;"><div class="hsR" style="top:0mm;left:0mm;margin-bottom:0mm;margin-right:0mm;width:50.36mm;height:77.19mm;display:inline-block;position:relative;vertical-align:middle;background-repeat:no-repeat;background-image:url('web_test_hd12.png');"></div></div></div></div></div></div><div class="hcD" style="left:1mm;top:87.15mm;width:132mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:132mm;"><span class="hrt cs1">[표&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:2.10mm;height:3.53mm;"><span class="hrt cs1">2</span></div><span class="hrt cs1">]</span></div></div></div></div></div><div class="hls ps30" style="padding-left:3.53mm;line-height:4.10mm;white-space:nowrap;left:0mm;top:97.78mm;height:4.94mm;width:134mm;"><span class="hrt cs26">4.3 &nbsp;EDA</span></div><div class="hls ps33" style="padding-left:3.53mm;line-height:56.80mm;white-space:nowrap;left:0mm;top:108.68mm;height:56.80mm;width:134mm;"><div class="hsG" style="width:64.54mm;height:51.27mm;display:inline-block;position:relative;vertical-align:middle;"><div class="hsR" style="left:0mm;margin-right:0mm;width:64.54mm;height:51.27mm;background-repeat:no-repeat;background-image:url('web_test_hd13.png');"></div><div class="hcD" style="left:0mm;top:53.27mm;width:64.54mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:64.54mm;"><span class="hrt cs1">[그림&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:4.21mm;height:3.53mm;"><span class="hrt cs1">17</span></div><span class="hrt cs1">]</span></div></div></div></div></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:169.89mm;height:3.53mm;width:134mm;"><span class="hrt cs6">데이터셋의 각 변수들 간의 상관관계를 시각적으로 보여주는 Heatmap을 생성</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:176.24mm;height:3.53mm;width:134mm;"><span class="hrt cs6">해본다. 색상의 변화가 일정해 보이는 것을 확인해 볼 수 있다.</span></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:159.94mm;top:33.53mm;width:4.06mm;height:3.53mm;"><span class="hrt cs0">20</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hcI"><div class="hls ps32" style="padding-left:3.53mm;line-height:4.10mm;white-space:nowrap;left:0mm;top:1.52mm;height:4.94mm;width:134mm;"><span class="hrt cs7">4.4 모델링 (Modeling)</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:12.24mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Modeling 과정은 Python의 AutoML 라이브러리인 Pycaret을 사용하여 모델&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:18.59mm;height:3.53mm;width:134mm;"><span class="hrt cs6">간 비교를 통해 해당 데이터에서 좋은 성능을 기대할 수 있는 상위 5개의 모델을&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:24.94mm;height:3.53mm;width:134mm;"><span class="hrt cs6">선정하고 그 모델들을 앙상블한 모델들을 만들어서 Accuracy의 비료를 통해 결</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:31.29mm;height:3.53mm;width:134mm;"><span class="hrt cs6">과적으로 개별 모델과 앙상블 모델 중 어떤 모델이 더 좋은 성능을 내는지를 알</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:37.64mm;height:3.53mm;width:134mm;"><span class="hrt cs6">아보고자 한다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:45.76mm;height:3.53mm;width:134mm;"><span class="hrt cs6">이때 데이터의 Target 변수는 기존의 19개의 종류로 나뉘는 천체 유형에서 데</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:52.11mm;height:3.53mm;width:134mm;"><span class="hrt cs6">이터의 양이 비슷한 SPECTROPHOTO_STD와 REDDEN_STD를 골라서 이진분류&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:58.46mm;height:3.53mm;width:134mm;"><span class="hrt cs6">데이터로 만들어주었다. 이때 데이터를 이진분류로 만들어준 이유는 Target 변수</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:64.81mm;height:3.53mm;width:134mm;"><span class="hrt cs6">가 19개나 되니까 실행시간이 굉장히 오래 걸려서 시간이 제한적인 현 상황에서</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:71.16mm;height:3.53mm;width:134mm;"><span class="hrt cs6">는 이진분류로 만들어주어서 실행시간을 줄이는 것이 낫다고 판단을 하여서이고,&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:77.51mm;height:3.53mm;width:134mm;"><span class="hrt cs6">데이터의 양이 비슷한 SPECTROPHOTO_STD와 REDDEN_STD로 종속변수를 고</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:83.86mm;height:3.53mm;width:134mm;"><span class="hrt cs6">른 이유는 데이터의 분포가 한 쪽으로 치우쳐져 있으면 모델이 정확한 예측을 하</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:90.21mm;height:3.53mm;width:134mm;"><span class="hrt cs6">기 어렵기 때문이다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:98.32mm;height:3.53mm;width:134mm;"><span class="hrt cs25">4.4.1 Model setup</span></div><div class="hls ps33" style="padding-left:3.53mm;line-height:14.91mm;white-space:nowrap;left:0mm;top:106.61mm;height:14.91mm;width:134mm;"><div class="hsG" style="width:134mm;height:9.39mm;display:inline-block;position:relative;vertical-align:middle;"><div class="hsR" style="left:0mm;margin-right:0mm;width:134mm;height:9.39mm;background-repeat:no-repeat;background-image:url('web_test_hd14.png');"></div><div class="hcD" style="left:0mm;top:11.38mm;width:134mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:134mm;"><span class="hrt cs1">[그림&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:4.21mm;height:3.53mm;"><span class="hrt cs1">18</span></div><span class="hrt cs1">]</span></div></div></div></div></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:125.93mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Pycaret을 사용해 단일 모델 성능을 비교하기 전 모델의 setup을 진행하여 해</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:132.28mm;height:3.53mm;width:134mm;"><span class="hrt cs6">당 데이터에서 좋은 성능을 낼 것으로 기대되는 개별 모델을 찾는다. 고정된 성능&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:138.63mm;height:3.53mm;width:134mm;"><span class="hrt cs6">비교를 위해 session_id 값을 42로 고정시키고 Target 변수는 ‘type’으로 설정하</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:144.98mm;height:3.53mm;width:134mm;"><span class="hrt cs6">며 use_gpu = True로 두어서 GPU를 사용하여 학습을 수행한다. 이는 gpu를 사</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:151.33mm;height:3.53mm;width:134mm;"><span class="hrt cs6">용해야 실행이 되는 모델들을 위해서 설정한 것이다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:159.44mm;height:3.53mm;width:134mm;"></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:167.56mm;height:3.53mm;width:134mm;"></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:175.67mm;height:3.53mm;width:134mm;"></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:30mm;top:33.53mm;width:4.06mm;height:3.53mm;"><span class="hrt cs0">21</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hcI"><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:1.59mm;height:3.53mm;width:134mm;"><span class="hrt cs25">4.4.2 Model comparison</span></div><div class="hls ps33" style="padding-left:3.53mm;line-height:95.19mm;white-space:nowrap;left:0mm;top:9.88mm;height:95.19mm;width:134mm;"><div class="hsG" style="width:134mm;height:89.67mm;display:inline-block;position:relative;vertical-align:middle;"><div class="hsR" style="left:0mm;margin-right:0mm;width:134mm;height:89.67mm;background-repeat:no-repeat;background-image:url('web_test_hd15.png');"></div><div class="hcD" style="left:0mm;top:91.67mm;width:134mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:134mm;"><span class="hrt cs1">[그림&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:4.21mm;height:3.53mm;"><span class="hrt cs1">19</span></div><span class="hrt cs1">]</span></div></div></div></div></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:109.48mm;height:3.53mm;width:134mm;"><span class="hrt cs6">compare_models()를 실행한 결과 Accuracy가 높을 것으로 예상되는 모델들의&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:115.83mm;height:3.53mm;width:134mm;"><span class="hrt cs6">결과가 위부터 순서대로 나왔다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:123.94mm;height:3.53mm;width:134mm;"><span class="hrt cs6">따라서 우리는 상위 5개 모델인 Random Forest Classifier, Gradient&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:130.29mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Boosting Classifier, CatBoost Classifier, Light Gradient Boosting Machine,&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:136.64mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Extreme Gradient Boosting를 개별 모델로 선정하고 이 모델들을 Blending,&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:142.99mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Stacking한 모델을 만들어서 비교한다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:151.11mm;height:3.53mm;width:134mm;"><span class="hrt cs6">일단 train set에서 10-fold 교차 검증을 수행했을 때 즉, 데이터를 10개의 부</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:157.46mm;height:3.53mm;width:134mm;"><span class="hrt cs6">분으로 나눈 뒤, 각 부분을 한 번씩 test set으로 사용하여 총 10번의 학습과 검</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:163.81mm;height:3.53mm;width:134mm;"><span class="hrt cs6">증을 수행한 후에 도출된 개별 모델들의 Accuracy의 평균을 확인해보면 아래와&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:170.16mm;height:3.53mm;width:134mm;"><span class="hrt cs6">같이 나온다.&nbsp;</span></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:159.94mm;top:33.53mm;width:4.06mm;height:3.53mm;"><span class="hrt cs0">22</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hcI"><div class="hls ps33" style="padding-left:3.53mm;line-height:58.05mm;white-space:nowrap;left:0mm;top:1.76mm;height:58.05mm;width:134mm;"><div class="hsG" style="width:120.85mm;height:52.53mm;display:inline-block;position:relative;vertical-align:middle;"><div class="hsR" style="left:0mm;margin-right:0mm;width:120.85mm;height:52.53mm;background-repeat:no-repeat;background-image:url('web_test_hd16.png');"></div><div class="hcD" style="left:0mm;top:54.52mm;width:120.85mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:120.85mm;"><span class="hrt cs1">[그림&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:4.21mm;height:3.53mm;"><span class="hrt cs1">20</span></div><span class="hrt cs1">]</span></div></div></div></div></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:64.22mm;height:3.53mm;width:134mm;"><span class="hrt cs6">위 결과에서 우리의 평가지표인 Accuracy만 본다면 Gradient Boosting&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:70.57mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Classifier, Random Forest, CatBoost Classifier, Light Gradient Boosting&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:76.92mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Machine, Extreme Gradient Boosting순으로 높게 나왔음을 알 수 있다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:85.04mm;height:3.53mm;width:134mm;"></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:93.15mm;height:3.53mm;width:134mm;"><span class="hrt cs25">4.4.3 Model Ensemble</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:101.26mm;height:3.53mm;width:134mm;"><span class="hrt cs6">이제 이 모델들을 앙상블(Ensemble)한 모델을 만들어서 성능비교를 해보겠다.</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:107.61mm;height:3.53mm;width:134mm;"><span class="hrt cs25">앙상블은 여러 개의 모델을 결합하여 더 좋은 성능을 가지는 모델을 만들어내는&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:113.96mm;height:3.53mm;width:134mm;"><span class="hrt cs25">기법이다.</span><span class="hrt cs6">&nbsp;이러한 앙상블 기법 중 우리는 Blending과 Stacking을 선택하였다. 이</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:120.31mm;height:3.53mm;width:134mm;"><span class="hrt cs6">때 Blending은 여러 개의 다른 모델들의 예측 결과를 가중 평균이나 투표 등의&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:126.66mm;height:3.53mm;width:134mm;"><span class="hrt cs6">방식으로 결합하는 방법을 말하며 Stacking은 Blending과 유사하게 모델들의 예</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:133.01mm;height:3.53mm;width:134mm;"><span class="hrt cs6">측 결과를 결합하지만, 이를 결합하는 방식이 다르다. Stacking에서는 기본 모델</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:139.36mm;height:3.53mm;width:134mm;"><span class="hrt cs6">의 예측 결과를 새로운 '메타 모델'의 입력으로 사용하는데, 메타 모델은 기본 모</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:145.71mm;height:3.53mm;width:134mm;"><span class="hrt cs6">델의 예측을 입력으로 받아 최종 예측을 생성하는 역할을 한다. 이렇게 하면 기본&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:152.06mm;height:3.53mm;width:134mm;"><span class="hrt cs6">모델의 예측 결과 간의 복잡한 관계를 학습하여 더 좋은 성능을 얻을 수 있다. 하</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:158.41mm;height:3.53mm;width:134mm;"><span class="hrt cs6">지만 스태킹은 구현이 복잡하고, 과적합의 위험이 존재한다.</span></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:30mm;top:33.53mm;width:4.06mm;height:3.53mm;"><span class="hrt cs0">23</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hcI"><div class="hls ps33" style="padding-left:3.53mm;line-height:26.78mm;white-space:nowrap;left:0mm;top:1.76mm;height:26.78mm;width:134mm;"><div class="hsG" style="width:125.76mm;height:21.25mm;display:inline-block;position:relative;vertical-align:middle;"><div class="hsR" style="left:0mm;margin-right:0mm;width:125.76mm;height:21.25mm;background-repeat:no-repeat;background-image:url('web_test_hd17.png');"></div><div class="hcD" style="left:0mm;top:23.25mm;width:125.76mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:125.76mm;"><span class="hrt cs1">[그림&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:4.21mm;height:3.53mm;"><span class="hrt cs1">21</span></div><span class="hrt cs1">]</span></div></div></div></div></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:32.95mm;height:3.53mm;width:134mm;"><span class="hrt cs6">위와 같은 코드를 이용해서 Blending 모델과 Stacking 모델을 만들었다. 이때&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:39.30mm;height:3.53mm;width:134mm;"><span class="hrt cs6">블렌딩 모델의 method = 'soft'로 두어 모델들이 예측한 확률 값을 가중 평균하여&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:45.65mm;height:3.53mm;width:134mm;"><span class="hrt cs6">최종 예측을 하게 된다. 그리고 train set에서 Accuracy를 확인해봤더니 아래와&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:52mm;height:3.53mm;width:134mm;"><span class="hrt cs6">같은 결과가 나왔다.</span></div><div class="hls ps33" style="padding-left:3.53mm;line-height:42.69mm;white-space:nowrap;left:0mm;top:60.29mm;height:42.69mm;width:134mm;"><div class="hsG" style="width:105.92mm;height:37.17mm;display:inline-block;position:relative;vertical-align:middle;"><div class="hsR" style="left:0mm;margin-right:0mm;width:105.92mm;height:37.17mm;background-repeat:no-repeat;background-image:url('web_test_hd18.png');"></div><div class="hcD" style="left:0mm;top:39.17mm;width:105.92mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:105.92mm;"><span class="hrt cs1">[그림&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:4.21mm;height:3.53mm;"><span class="hrt cs1">22</span></div><span class="hrt cs1">]</span></div></div></div></div></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:107.40mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Voting Classifier과 Stacking Classifier 중 train set에서만 예측을 해봤을 때&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:113.75mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Voting Classifier 즉, Blending한 모델이 더 높은 정확도가 나왔음을 알 수 있다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:4.10mm;white-space:nowrap;left:0mm;top:121.79mm;height:4.94mm;width:134mm;"></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:159.94mm;top:33.53mm;width:4.06mm;height:3.53mm;"><span class="hrt cs0">24</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hcI"><div class="hls ps32" style="padding-left:3.53mm;line-height:4.10mm;white-space:nowrap;left:0mm;top:1.52mm;height:4.94mm;width:134mm;"><span class="hrt cs7">4.5 모델 분석 (Model Analysis)</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:12.24mm;height:3.53mm;width:134mm;"><span class="hrt cs6">이후 pycaret의 plot_model(model, plot = 'feature')을 통해서 개별 모델들의&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:18.59mm;height:3.53mm;width:134mm;"><span class="hrt cs6">변수 중요도를 파악한다.</span></div></div></div><div class="htb" style="left:31mm;width:134mm;top:64.77mm;height:151.84mm;"><svg class="hs" viewBox="-2.50 -2.50 139 156.84" style="left:-2.50mm;top:-2.50mm;width:139mm;height:156.84mm;"><path d="M0,0 L0,149.85" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M132.01,0 L132.01,149.85" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,0 L132.01,0" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,4.52 L132.01,4.52" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,74.26 L132.01,74.26" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,78.78 L132.01,78.78" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,149.84 L132.01,149.84" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M132.01,0 L132.01,149.85" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,0 L0,149.85" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,149.84 L132.01,149.84" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,0 L132.01,0" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path></svg><div class="hce" style="left:0mm;top:0mm;width:132.01mm;height:4.52mm;"><div class="hcD" style="left:1.80mm;top:0.50mm;"><div class="hcI"><div class="hls ps28" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:128.40mm;"><span class="hrt cs6">Random Forest Classifier</span></div></div></div></div><div class="hce" style="left:0mm;top:4.52mm;width:132.01mm;height:69.74mm;"><div class="hcD" style="left:1.80mm;top:0.50mm;"><div class="hcI"><div class="hls ps13" style="line-height:68.74mm;white-space:nowrap;left:0mm;top:0mm;height:68.74mm;width:128.40mm;"><div class="hsR" style="top:0mm;left:0mm;margin-bottom:0mm;margin-right:0mm;width:110mm;height:68.74mm;display:inline-block;position:relative;vertical-align:middle;background-repeat:no-repeat;background-image:url('web_test_hd19.png');"></div></div></div></div></div><div class="hce" style="left:0mm;top:74.26mm;width:132.01mm;height:4.52mm;"><div class="hcD" style="left:1.80mm;top:0.50mm;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:128.40mm;"><span class="hrt cs6">Gradient Boosting Classifier</span></div></div></div></div><div class="hce" style="left:0mm;top:78.78mm;width:132.01mm;height:71.06mm;"><div class="hcD" style="left:1.80mm;top:0.50mm;"><div class="hcI"><div class="hls ps13" style="line-height:70.07mm;white-space:nowrap;left:0mm;top:0mm;height:70.07mm;width:128.40mm;"><div class="hsR" style="top:0mm;left:0mm;margin-bottom:0mm;margin-right:0mm;width:112.12mm;height:70.06mm;display:inline-block;position:relative;vertical-align:middle;background-repeat:no-repeat;background-image:url('web_test_hd1a.png');"></div></div></div></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:30mm;top:33.53mm;width:4.06mm;height:3.53mm;"><span class="hrt cs0">25</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hcI"></div></div><div class="htb" style="left:31mm;width:134mm;top:41mm;height:177.74mm;"><svg class="hs" viewBox="-2.50 -2.50 139 182.74" style="left:-2.50mm;top:-2.50mm;width:139mm;height:182.74mm;"><path d="M0,0 L0,171.23" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M132.01,0 L132.01,171.23" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,0 L132.01,0" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,4.52 L132.01,4.52" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,85.46 L132.01,85.46" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,89.98 L132.01,89.98" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,171.22 L132.01,171.22" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,175.75 L132.01,175.75" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M132.01,0 L132.01,171.23" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,0 L0,171.23" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,175.75 L132.01,175.75" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,0 L132.01,0" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path></svg><div class="hce" style="left:0mm;top:0mm;width:132.01mm;height:4.52mm;"><div class="hcD" style="left:1.80mm;top:0.50mm;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:128.40mm;"><span class="hrt cs6">CatBoost Classifier</span></div></div></div></div><div class="hce" style="left:0mm;top:4.52mm;width:132.01mm;height:80.93mm;"><div class="hcD" style="left:1.80mm;top:0.50mm;"><div class="hcI"><div class="hls ps13" style="line-height:79.94mm;white-space:nowrap;left:0mm;top:0mm;height:79.94mm;width:128.40mm;"><div class="hsR" style="top:0mm;left:0mm;margin-bottom:0mm;margin-right:0mm;width:128.41mm;height:79.94mm;display:inline-block;position:relative;vertical-align:middle;background-repeat:no-repeat;background-image:url('web_test_hd1b.png');"></div></div></div></div></div><div class="hce" style="left:0mm;top:85.46mm;width:132.01mm;height:4.52mm;"><div class="hcD" style="left:1.80mm;top:0.50mm;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:128.40mm;"><span class="hrt cs6">Light Gradient Boosting Machine</span></div></div></div></div><div class="hce" style="left:0mm;top:89.98mm;width:132.01mm;height:81.24mm;"><div class="hcD" style="left:1.80mm;top:0.50mm;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:0mm;"></div></div></div><div class="hsR" style="top:0.50mm;left:1.80mm;width:128.41mm;height:80.25mm;background-repeat:no-repeat;background-image:url('web_test_hd1c.png');"></div></div><div class="hce" style="left:0mm;top:171.22mm;width:132.01mm;height:4.52mm;"><div class="hcD" style="left:1.80mm;top:0.50mm;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:128.40mm;"></div></div></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:159.94mm;top:33.53mm;width:4.06mm;height:3.53mm;"><span class="hrt cs0">26</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hfS" style="left:0mm;top:175.63mm;width:50mm;height:0.11mm;"><svg class="hs" viewBox="-0.12 -0.12 50.23 0.35" style="left:-0.12mm;top:-0.12mm;width:50.23mm;height:0.35mm;left:0;top:0;"><path d="M0,0.06 L50,0.06" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path></svg></div><div class="hcD" style="left:0mm;top:177.68mm;"><div class="hcI"><div class="hls ps1" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:-0.16mm;height:3.17mm;width:134mm;"><div class="haN" style="left:0mm;top:0mm;width:4.71mm;height:3.17mm;"><span class="hrt cs3">11)</span></div><span class="hrt cs29">&nbsp;&nbsp;comfusion matrix는 모델에서 구한 분류의 예측값과 데이터의 실제값의 발생빈도를 나열한 행</span></div><div class="hls ps1" style="padding-left:4.62mm;line-height:2.48mm;white-space:nowrap;left:0mm;top:3.98mm;height:3.17mm;width:134mm;"><span class="hrt cs29">렬을 뜻하며 분류모델에서 예측결과를 평가하는데 사용되는 행렬이다.</span></div></div></div><div class="hcI"><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:86.43mm;height:3.53mm;width:134mm;"></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:94.54mm;height:3.53mm;width:134mm;"><span class="hrt cs6">이 결과를 통해서 Random Forest 모델에서는 psfMag_g, fiberMag_g,&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:100.89mm;height:3.53mm;width:134mm;"><span class="hrt cs6">petroMag_u 순으로 중요한 변수라고 나왔고, Catboost 모델에서는 fiberID,&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:107.24mm;height:3.53mm;width:134mm;"><span class="hrt cs6">modelMag_g, fibermag_z순으로 Light Gradient Boosting 모델에서는 fiberID,&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:113.59mm;height:3.53mm;width:134mm;"><span class="hrt cs6">ID, fibermag_z가 중요한 모델이라는 결과가 나왔다. 이때까지 모델들은 각 변수</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:119.94mm;height:3.53mm;width:134mm;"><span class="hrt cs6">마다 중요도가 큰 차이가 나지 않았는데 Gradient Boosting 모델과 Extreme&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:126.29mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Gradient Boosting 모델에서는 fiberMag_g 변수가 다른 변수들에 비해서 중요도</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:132.64mm;height:3.53mm;width:134mm;"><span class="hrt cs6">가 압도적으로 높음을 볼 수가 있다. 이에 대한 이유는 변수들에 대해서 더 파악</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:138.99mm;height:3.53mm;width:134mm;"><span class="hrt cs6">하고 이해를 한 후에 알 수 있을 것 같다. 또한 위 변수들 중에서 fiberMag_g가&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:145.34mm;height:3.53mm;width:134mm;"><span class="hrt cs6">대부분의 모델에서 중요한 변수로 꼽힌다는 점은 눈여겨 볼만한 내용이라고 생각</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:151.69mm;height:3.53mm;width:134mm;"><span class="hrt cs6">한다. 다음은</span><span class="hrt cs25">&nbsp;comfusion matrix</span><span class="hfN" style="top:-1.76mm;"><span class="hrt cs25" style="font-size:5pt;top:-1pt;">11)</span></span><span class="hrt cs6">를 사용하여 모델의 정확도를 측정한 내용을 시</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:158.04mm;height:3.53mm;width:134mm;"><span class="hrt cs6">각화하여 보여준다.</span></div></div></div><div class="htG" style="left:30mm;width:134mm;top:40mm;height:86.60mm;"><div class="htb" style="left:1mm;width:132.01mm;top:1mm;height:78.08mm;"><svg class="hs" viewBox="-2.50 -2.50 137 83.08" style="left:-2.50mm;top:-2.50mm;width:137mm;height:83.08mm;"><path d="M0,0 L0,78.08" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M132.01,0 L132.01,78.08" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,0 L132.01,0" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,4.52 L132.01,4.52" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,78.08 L132.01,78.08" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M132.01,0 L132.01,78.08" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,0 L0,78.08" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,78.08 L132.01,78.08" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path><path d="M0,0 L132.01,0" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path></svg><div class="hce" style="left:0mm;top:0mm;width:132.01mm;height:4.52mm;"><div class="hcD" style="left:1.80mm;top:0.50mm;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:128.40mm;"><span class="hrt cs6">Extreme Gradient Boosting</span></div></div></div></div><div class="hce" style="left:0mm;top:4.52mm;width:132.01mm;height:73.56mm;"><div class="hcD" style="left:1.80mm;top:0.50mm;"><div class="hcI"><div class="hls ps13" style="line-height:72.56mm;white-space:nowrap;left:0mm;top:0mm;height:72.56mm;width:128.40mm;"><div class="hsR" style="top:0mm;left:0mm;margin-bottom:0mm;margin-right:0mm;width:116.11mm;height:72.56mm;display:inline-block;position:relative;vertical-align:middle;background-repeat:no-repeat;background-image:url('web_test_hd1d.png');"></div></div></div></div></div></div><div class="hcD" style="left:1mm;top:82.08mm;width:132.01mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:132mm;"><span class="hrt cs1">[표&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:2.10mm;height:3.53mm;"><span class="hrt cs1">3</span></div><span class="hrt cs1">]</span></div></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:30mm;top:33.53mm;width:4.06mm;height:3.53mm;"><span class="hrt cs0">27</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hcI"><div class="hls ps33" style="padding-left:3.53mm;line-height:52.82mm;white-space:nowrap;left:0mm;top:1.76mm;height:52.82mm;width:134mm;"><div class="hsG" style="width:67.34mm;height:47.29mm;display:inline-block;position:relative;vertical-align:middle;"><div class="hsR" style="left:0mm;margin-right:0mm;width:67.34mm;height:47.29mm;background-repeat:no-repeat;background-image:url('web_test_hd1e.png');"></div><div class="hcD" style="left:0mm;top:49.29mm;width:67.34mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:67.34mm;"><span class="hrt cs1">[그림 23]</span></div></div></div></div></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:58.99mm;height:3.53mm;width:134mm;"><span class="hrt cs6">TP는 참이라고 예측했는데 실제 값이 참인 것의 비율</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:67.11mm;height:3.53mm;width:134mm;"><span class="hrt cs6">FN은 거짓이라고 예측했는데 실제 값이 참인 비율</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:75.22mm;height:3.53mm;width:134mm;"><span class="hrt cs6">FP는 참이라고 예측했는데 실제 값이 거짓일 비율</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:83.33mm;height:3.53mm;width:134mm;"><span class="hrt cs6">TN은 거짓이라고 예측했는데 실제 값이 거짓일 비율이다.</span></div><div class="hls ps33" style="padding-left:3.53mm;line-height:9.41mm;white-space:nowrap;left:0mm;top:91.62mm;height:9.74mm;width:134mm;"><div class="heq" style="width:57.59mm;height:9.74mm;background-repeat:no-repeat;background-image:url('web_test_hd1f.png');display:inline-block;position:relative;vertical-align:middle;"></div></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:105.77mm;height:3.53mm;width:134mm;"><span class="hrt cs6">이때의 정확도는 (모든 확률 중에서 참이라고 예측했는데 실제 값이 참인 것의&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:112.12mm;height:3.53mm;width:134mm;"><span class="hrt cs6">비율) + (TN은 거짓이라고 예측했는데 실제 값이 거짓일 비율)이다. 즉 예상한&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:118.47mm;height:3.53mm;width:134mm;"><span class="hrt cs6">값이 맞았을 때 정확도가 높은 것이다. Random Forest Classifier의 comfusion&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:124.82mm;height:3.53mm;width:134mm;"><span class="hrt cs6">matrix로 예를 들어보면</span></div><div class="hls ps33" style="padding-left:3.53mm;line-height:49.64mm;white-space:nowrap;left:0mm;top:133.11mm;height:49.64mm;width:134mm;"><div class="hsG" style="width:72.77mm;height:44.12mm;display:inline-block;position:relative;vertical-align:middle;"><div class="hsR" style="left:0mm;margin-right:0mm;width:72.77mm;height:44.12mm;background-repeat:no-repeat;background-image:url('web_test_hd20.png');"></div><div class="hcD" style="left:0mm;top:46.12mm;width:72.77mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:72.77mm;"><span class="hrt cs1">[그림 24]</span></div></div></div></div></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:159.94mm;top:33.53mm;width:4.06mm;height:3.53mm;"><span class="hrt cs0">28</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hcI"><div class="hls ps32" style="padding-left:3.53mm;line-height:3.43mm;white-space:nowrap;left:0mm;top:1.76mm;height:4.23mm;width:134mm;"><span class="hrt cs6">모든 사각형 안의 값 =&nbsp;</span><div class="heq" style="width:56.53mm;height:4.23mm;background-repeat:no-repeat;background-image:url('web_test_hd21.png');display:inline-block;position:relative;vertical-align:middle;"></div></div><div class="hls ps32" style="padding-left:3.53mm;line-height:3.43mm;white-space:nowrap;left:0mm;top:10.58mm;height:4.23mm;width:134mm;"><span class="hrt cs6">초록색 사각형 안의 값 =&nbsp;</span><div class="heq" style="width:37.23mm;height:4.23mm;background-repeat:no-repeat;background-image:url('web_test_hd22.png');display:inline-block;position:relative;vertical-align:middle;"></div></div><div class="hls ps32" style="padding-left:3.53mm;line-height:9.41mm;white-space:nowrap;left:0mm;top:19.40mm;height:9.74mm;width:134mm;"><span class="hrt cs6">이때 정확도는&nbsp;</span><div class="heq" style="width:28.63mm;height:9.74mm;background-repeat:no-repeat;background-image:url('web_test_hd23.png');display:inline-block;position:relative;vertical-align:middle;"></div><span class="hrt cs6">&nbsp;이고 이는 위에서 구했던 Random Forest&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:31.79mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Classifier의 값과 정확히 일치한다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:4.10mm;white-space:nowrap;left:0mm;top:39.83mm;height:4.94mm;width:134mm;"></div><div class="hls ps32" style="padding-left:3.53mm;line-height:4.10mm;white-space:nowrap;left:0mm;top:50.48mm;height:4.94mm;width:134mm;"><span class="hrt cs7">4.6 모델 평가 (Model Evaluation)</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:61.21mm;height:3.53mm;width:134mm;"><span class="hrt cs6">지금까지는 train set에서만 테스트를 하면서 이에 해당하는 Accuracy를 구했</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:67.56mm;height:3.53mm;width:134mm;"><span class="hrt cs6">지만 실제 우리가 수행해야 하는 건&nbsp;</span><span class="hrt cs25">학습된 train set을 통해서 test set의 값을&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:73.91mm;height:3.53mm;width:134mm;"><span class="hrt cs25">예측하는 것이다</span><span class="hrt cs6">. 따라서 pycaret의 finalize_model을 이용하여 평가를 하기 위한&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:80.26mm;height:3.53mm;width:134mm;"><span class="hrt cs6">최종 모델을 만든다.</span></div><div class="hls ps33" style="padding-left:3.53mm;line-height:47.39mm;white-space:nowrap;left:0mm;top:88.55mm;height:47.39mm;width:134mm;"><div class="htG" style="left:0mm;width:132.35mm;top:0mm;height:47.39mm;display:inline-block;position:relative;vertical-align:middle;"><div class="htb" style="left:1mm;width:130.36mm;top:1mm;height:38.87mm;"><div class="hce" style="left:0mm;top:0mm;width:43.34mm;height:12.45mm;"><div class="hcD" style="left:1.80mm;top:0.50mm;"><div class="hcI" style="top:0.66mm;"><div class="hls ps13" style="line-height:9.90mm;white-space:nowrap;left:0mm;top:0mm;height:10.13mm;width:39.74mm;"><div class="hsR" style="top:0mm;left:0mm;margin-bottom:0mm;margin-right:0mm;width:40.40mm;height:10.13mm;display:inline-block;position:relative;vertical-align:middle;background-repeat:no-repeat;background-image:url('web_test_hd24.png');"></div></div></div></div></div><div class="hce" style="left:43.34mm;top:0mm;width:43.35mm;height:12.45mm;"><div class="hcD" style="left:1.80mm;top:0.50mm;"><div class="hcI" style="top:0.84mm;"><div class="hls ps13" style="line-height:9.46mm;white-space:nowrap;left:0mm;top:0mm;height:9.78mm;width:39.74mm;"><div class="hsR" style="top:0mm;left:0mm;margin-bottom:0mm;margin-right:0mm;width:40.40mm;height:9.78mm;display:inline-block;position:relative;vertical-align:middle;background-repeat:no-repeat;background-image:url('web_test_hd25.png');"></div></div></div></div></div><div class="hce" style="left:86.68mm;top:0mm;width:43.67mm;height:12.45mm;"><div class="hcD" style="left:1.80mm;top:0.50mm;"><div class="hcI" style="top:0.81mm;"><div class="hls ps13" style="line-height:9.52mm;white-space:nowrap;left:0mm;top:0mm;height:9.83mm;width:40.08mm;"><div class="hsR" style="top:0mm;left:0mm;margin-bottom:0mm;margin-right:0mm;width:34.66mm;height:9.83mm;display:inline-block;position:relative;vertical-align:middle;background-repeat:no-repeat;background-image:url('web_test_hd26.png');"></div></div></div></div></div><div class="hce" style="left:0mm;top:12.45mm;width:65.18mm;height:13.24mm;"><div class="hcD" style="left:1.80mm;top:0.50mm;"><div class="hcI"><div class="hls ps13" style="line-height:12.25mm;white-space:nowrap;left:0mm;top:0mm;height:12.25mm;width:61.58mm;"><div class="hsR" style="top:0mm;left:0mm;margin-bottom:0mm;margin-right:0mm;width:55.91mm;height:12.25mm;display:inline-block;position:relative;vertical-align:middle;background-repeat:no-repeat;background-image:url('web_test_hd27.png');"></div></div></div></div></div><div class="hce" style="left:65.18mm;top:12.45mm;width:65.18mm;height:13.24mm;"><div class="hcD" style="left:1.80mm;top:0.50mm;"><div class="hcI" style="top:0.30mm;"><div class="hls ps13" style="line-height:11.64mm;white-space:nowrap;left:0mm;top:0mm;height:11.64mm;width:61.58mm;"><div class="hsR" style="top:0mm;left:0mm;margin-bottom:0mm;margin-right:0mm;width:49.49mm;height:11.64mm;display:inline-block;position:relative;vertical-align:middle;background-repeat:no-repeat;background-image:url('web_test_hd28.png');"></div></div></div></div></div><div class="hce" style="left:0mm;top:25.70mm;width:65.18mm;height:13.17mm;"><div class="hcD" style="left:1.80mm;top:0.50mm;"><div class="hcI" style="top:0.01mm;"><div class="hls ps13" style="line-height:12.16mm;white-space:nowrap;left:0mm;top:0mm;height:12.16mm;width:61.58mm;"><div class="hsR" style="top:0mm;left:0mm;margin-bottom:0mm;margin-right:0mm;width:39.42mm;height:12.16mm;display:inline-block;position:relative;vertical-align:middle;background-repeat:no-repeat;background-image:url('web_test_hd29.png');"></div></div></div></div></div><div class="hce" style="left:65.18mm;top:25.70mm;width:65.18mm;height:13.17mm;"><div class="hcD" style="left:1.80mm;top:0.50mm;"><div class="hcI"><div class="hls ps13" style="line-height:12.17mm;white-space:nowrap;left:0mm;top:0mm;height:12.17mm;width:61.58mm;"><div class="hsR" style="top:0mm;left:0mm;margin-bottom:0mm;margin-right:0mm;width:35.94mm;height:12.17mm;display:inline-block;position:relative;vertical-align:middle;background-repeat:no-repeat;background-image:url('web_test_hd2a.png');"></div></div></div></div></div></div><div class="hcD" style="left:1mm;top:42.86mm;width:130.36mm;height:3.53mm;overflow:hidden;"><div class="hcI"><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:130.36mm;"><span class="hrt cs1">[표&nbsp;</span><div class="haN" style="left:0mm;top:0mm;width:2.10mm;height:3.53mm;"><span class="hrt cs1">4</span></div><span class="hrt cs1">]</span></div></div></div></div></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:140.35mm;height:3.53mm;width:134mm;"><span class="hrt cs6">train set으로만 Accuracy를 확인했을 때는 Gradient Boosting Classifier,&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:146.70mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Random Forest, Voting Classifier, CatBoost Classifier, Light Gradient&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:153.05mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Boosting Machine, Stacking Classifier, Extreme Gradient Boosting순으로 높</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:159.40mm;height:3.53mm;width:134mm;"><span class="hrt cs6">았는데 실제 데이터를 통해서 분류과정을 거쳐보니 Light Gradient Boosting&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:165.75mm;height:3.53mm;width:134mm;"><span class="hrt cs6">Machine, Stacking Classifier, Voting Classifier, Extreme Gradient Boosting,&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:172.10mm;height:3.53mm;width:134mm;"><span class="hrt cs6">CatBoost Classifier, Random Forest, Gradient Boosting Classifier순으로 순서</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:178.45mm;height:3.53mm;width:134mm;"><span class="hrt cs6">가 바뀜을 확인 할 수 있다.</span></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:30mm;top:33.53mm;width:4.06mm;height:3.53mm;"><span class="hrt cs0">29</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hfS" style="left:0mm;top:171.49mm;width:50mm;height:0.11mm;"><svg class="hs" viewBox="-0.12 -0.12 50.23 0.35" style="left:-0.12mm;top:-0.12mm;width:50.23mm;height:0.35mm;left:0;top:0;"><path d="M0,0.06 L50,0.06" style="stroke:#000000;stroke-linecap:butt;stroke-width:0.12;"></path></svg></div><div class="hcD" style="left:0mm;top:173.55mm;"><div class="hcI"><div class="hls ps1" style="line-height:2.48mm;white-space:nowrap;left:0mm;top:-0.16mm;height:3.17mm;width:134mm;"><div class="haN" style="left:0mm;top:0mm;width:4.71mm;height:3.17mm;"><span class="hrt cs3">12)</span></div><span class="hrt cs3">&nbsp;앞에서 선정한 5가지 모델인&nbsp;</span><span class="hrt cs29">Random Forest Classifier, Gradient Boosting Classifier,&nbsp;</span></div><div class="hls ps1" style="padding-left:4.62mm;line-height:2.48mm;white-space:nowrap;left:0mm;top:3.98mm;height:3.17mm;width:134mm;"><span class="hrt cs29">CatBoost Classifier, Light Gradient Boosting Machine, Extreme Gradient Boosting를 의미한</span></div><div class="hls ps1" style="padding-left:4.62mm;line-height:2.48mm;white-space:nowrap;left:0mm;top:8.11mm;height:3.17mm;width:134mm;"><span class="hrt cs29">다.</span></div></div></div><div class="hcI"><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:1.59mm;height:3.53mm;width:134mm;"><span class="hrt cs6">따라서 이 데이터에서는 Light Gradient Boosting Machine이 가장 높은 정확</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:7.94mm;height:3.53mm;width:134mm;"><span class="hrt cs6">도를 보임을 알 수 있다. 또한 개별 모델들은 모델마다 정확도의 차이가 있는데&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:14.29mm;height:3.53mm;width:134mm;"><span class="hrt cs6">앙상블 모델의 경우 Blending한 모델과 Stacking한 모델이 각각 0.9810과&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:20.64mm;height:3.53mm;width:134mm;"><span class="hrt cs6">0.9812로 높은 성능과 비슷한 정확도를 가짐을 알 수 있다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:28.75mm;height:3.53mm;width:134mm;"></div><div class="hls ps32" style="padding-left:3.53mm;line-height:4.10mm;white-space:nowrap;left:0mm;top:36.79mm;height:4.94mm;width:134mm;"><span class="hrt cs7">5. 결론 및 향후 계획</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:47.52mm;height:3.53mm;width:134mm;"><span class="hrt cs6">앙상블 모델이란 “여러 개의 개별 모델을 조합하여 최적의 모델로 일반화하는&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:53.87mm;height:3.53mm;width:134mm;"><span class="hrt cs6">방법”으로 대부분의 예측 문제나 분류 문제에서 개별 모델보다 성능이 좋게 나오</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:60.22mm;height:3.53mm;width:134mm;"><span class="hrt cs6">는 경우가 많다. 이번 분류 문제에 대해서도 앙상블 모델이 개별 모델보다 더 좋</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:66.57mm;height:3.53mm;width:134mm;"><span class="hrt cs6">은 성능을 낼 것으로 예상했지만 결과적으로 LightGBM모델, 즉 개별 모델이 더&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:72.92mm;height:3.53mm;width:134mm;"><span class="hrt cs6">좋은 성능을 냈다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:81.03mm;height:3.53mm;width:134mm;"><span class="hrt cs6">한편 앙상블 모델을 구성할 때 중요한 요소 중 하나는 weak classifier(분류율</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:87.38mm;height:3.53mm;width:134mm;"><span class="hrt cs6">이 50%를 조금 넘기는 분류기)들을 결합하여 각 classifier 들로부터 최대한 다양</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:93.73mm;height:3.53mm;width:134mm;"><span class="hrt cs6">한 결과 값을 얻어낸 후 각 모델에 대한 오류를 서로 상쇄시켜 모델의 전체적인&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:100.08mm;height:3.53mm;width:134mm;"><span class="hrt cs6">성능을 향상시키는 것이다. 그러나 이번 프로젝트에서 결합한 모델들은 결과 값이&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:106.43mm;height:3.53mm;width:134mm;"><span class="hrt cs6">가장 높은&nbsp;</span><span class="hfN" style="top:-1.76mm;"><span class="hrt cs6" style="font-size:5pt;top:-1pt;">12)</span></span><span class="hrt cs6">모델들 5가지를 선정한 것이므로 이미 개별 모델들의 성능이 매우&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:112.78mm;height:3.53mm;width:134mm;"><span class="hrt cs6">뛰어나서 앙상블의 효과를 얻기 어려웠던 것으로 추정된다. 따라서 다음에 다시&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:119.13mm;height:3.53mm;width:134mm;"><span class="hrt cs6">앙상블 모델을 구성해본다면 weak classifier의 기준에 맞는 모델들을 선정하여&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:125.48mm;height:3.53mm;width:134mm;"><span class="hrt cs6">(이 데이터를 예로 들면 Quadratic Discriminant Analysis, Logistic Regression,&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:131.83mm;height:3.53mm;width:134mm;"><span class="hrt cs6">K-Nearest Neighbors, Naive Bayes, Dummy Classifier) 앙상블 모델을 구성하</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:138.18mm;height:3.53mm;width:134mm;"><span class="hrt cs6">면 더욱 좋은 성능을 낼 것으로 기대된다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:146.30mm;height:3.53mm;width:134mm;"></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:154.41mm;height:3.53mm;width:134mm;"><span class="hrt cs6">&nbsp;pycaret을 이용하여 하이퍼파라미터 튜닝을 하여 각 모델의 성능을 높였다면&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:160.76mm;height:3.53mm;width:134mm;"><span class="hrt cs6">각 모델에서 현재보다 더 높은 정확도를 가지게 되었을 것이다. 또한 프로젝트 전</span></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:159.94mm;top:33.53mm;width:4.06mm;height:3.53mm;"><span class="hrt cs0">30</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hcI"><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:-0.18mm;height:3.53mm;width:134mm;"><span class="hrt cs6">체적으로 더 높은 정확도를 가질 수 있었을 것으로 예상되는데, 그러지 못했던 것</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:6.17mm;height:3.53mm;width:134mm;"><span class="hrt cs6">에 아쉬움이 남는다.</span></div><div class="hls ps32" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:14.29mm;height:3.53mm;width:134mm;"><span class="hrt cs6">본 연구에서 진행한 머신러닝 성능 평가 연구가 우리 수학과 학생들과 데이터&nbsp;</span></div><div class="hls ps32" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:20.64mm;height:3.53mm;width:134mm;"><span class="hrt cs6">사이언스 입문자들에게 많은 도움이 되기를 바란다.</span></div></div></div></div><div class="hpa" style="width:210mm;height:297mm;"><div class="hpN" style="left:30mm;top:33.53mm;width:4.06mm;height:3.53mm;"><span class="hrt cs0">31</span></div><div class="hcD" style="left:30mm;top:40mm;"><div class="hcI"><div class="hls ps14" style="line-height:4.10mm;white-space:nowrap;left:0mm;top:-0.25mm;height:4.94mm;width:134mm;"><span class="hrt cs7">참고 문헌</span></div><div class="hls ps17" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:8.71mm;height:3.53mm;width:134mm;"><span class="hrt cs1">&nbsp;</span></div><div class="hls ps18" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:14.36mm;height:3.53mm;width:134mm;"><span class="hrt cs1">[1] PyCaret Docs,&nbsp;</span><span class="hrt cs6">https://pycaret.gitbook.io/docs</span></div><div class="hls ps18" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:20mm;height:3.53mm;width:134mm;"><span class="hrt cs6">[2] DACON, https://dacon.io/competitions/official/235573</span></div><div class="hls ps18" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:25.65mm;height:3.53mm;width:134mm;"><span class="hrt cs6">[3] SDSS, https://www.sdss4.org/dr17/algorithms</span></div><div class="hls ps18" style="padding-left:3.53mm;line-height:2.79mm;white-space:nowrap;left:0mm;top:31.29mm;height:3.53mm;width:134mm;"><span class="hrt cs1">[4] 유혜인,&nbsp;</span><span class="hrt cs25">SDSS 측광 자료를 이용한 우리 은하 내 구상성단들의 수평계열항</span></div><div class="hls ps18" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:36.94mm;height:3.53mm;width:134mm;"><span class="hrt cs25">성 연구</span><span class="hrt cs6">, 이화여자대학교 대학원, 2014.</span></div><div class="hls ps25" style="line-height:2.79mm;white-space:nowrap;left:3.53mm;top:42.58mm;height:3.53mm;width:130.47mm;"><span class="hrt cs1">[5] 권철민,&nbsp;</span><span class="hrt cs15">파이썬 머신러닝 완벽 가이드</span><span class="hrt cs1">, 위키북스, 2022.</span></div><div class="hls ps25" style="line-height:2.79mm;white-space:nowrap;left:3.53mm;top:48.22mm;height:3.53mm;width:130.47mm;"><span class="hrt cs1">[6]&nbsp;</span><span class="hrt cs6">오렐리앙 제롱,&nbsp;</span><span class="hrt cs25">핸즈온 머신러닝</span><span class="hrt cs6">, 박해선 옮김, 한빛미디어, 2023.</span></div><div class="hls ps25" style="line-height:2.79mm;white-space:nowrap;left:3.53mm;top:53.87mm;height:3.53mm;width:130.47mm;"><span class="hrt cs6">[7] 민성환,&nbsp;</span><span class="hrt cs25">부도 예측을 위한 앙상블 분류기 개발</span><span class="hrt cs6">, 한국산업정보학회논문지,&nbsp;</span></div><div class="hls ps25" style="padding-left:7.58mm;line-height:2.79mm;white-space:nowrap;left:3.53mm;top:59.51mm;height:3.53mm;width:130.47mm;"><span class="hrt cs6">2011</span></div><div class="hls ps25" style="line-height:2.79mm;white-space:nowrap;left:3.53mm;top:65.16mm;height:3.53mm;width:130.47mm;"><span class="hrt cs6">[8] 홍진혁 &amp; 조성배,&nbsp;</span><span class="hrt cs25">분류 성능 향상을 위한 다양성 기반 앙상블 유전자 프로</span></div><div class="hls ps25" style="padding-left:7.58mm;line-height:2.79mm;white-space:nowrap;left:3.53mm;top:70.80mm;height:3.53mm;width:130.47mm;"><span class="hrt cs25">그래밍</span><span class="hrt cs6">, 정보과학회논문지, 2005</span></div><div class="hls ps2" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:76.45mm;height:3.53mm;width:134mm;"></div><div class="hls ps13" style="line-height:2.79mm;white-space:nowrap;left:0mm;top:82.09mm;height:3.53mm;width:134mm;"></div></div></div></div></body></html>